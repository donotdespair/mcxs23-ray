---
title: "Effects of Fiscal Policy on Unemployment and Output in Australia: a Bayesian SVAR Approach"
author: "Ray Gomez"

execute:
  echo: false
  
bibliography: references.bib
---

> **Abstract.** This proposed study attempts to estimate the effects of various fiscal policy instruments on unemployment and output in Australia, using a Bayesian Structural Vector Autoregression approach. Impulse response functions and forecast variance error decomposition methods are expected to be used for measuring these effects. This study also compares two model specifications -- a basic model with unempolyment, output, fiscal policy, and monetary policy variables; and an extended model which also models heteroskedasticity due to the COVID-19 pandemic. 
>
> **Keywords.** Fiscal policy, unemployment, GDP, Australia impulse response, forecast variance error decomposition, bsvars, R

## Introduction

In many countries, fiscal policy is viewed as a direct means of achieving inclusive development objectives, frequently articulated as twin goals of sustained growth and low poverty (often achieved through low unemployment). However, fiscal policy shocks often occur in multifaceted ways, with simultaneous changes in both revenue and expenditure-side elements of the fiscal balance, making it difficult to disentangle the effects of any one particular policy shock.

This study attempts to identify the effect of the following fiscal policy instruments: tax policy, public investment expenditures, and social transfers, on both unemployment and output in the Australian context, using a Bayesian Structural Vector Autoregression (BSVAR) approach.

The analysis is guided by the work of Abubakar, Attahir B. (2016), which used an SVAR approach to estimate the effect of public expenditures and public revenues on Nigerian output and unemployment. This study extends his work by using a more extensive dataset, containing more disaggregated revenue and expenditure variables, the inclusion of monetary policy control variables, as well as through the use of Bayesian estimation.

## Data sources

Quarterly data from Q1 1990 to Q4 2022 was sourced from the Australian Bureau of Statistics (ABS) and the Reserve Bank of Australia (RBA) and extracted using the `readabs` and `readrba` packages in `R`.

Unemployment rate and nominal GDP data are viewed as the outcome variables while tax revenue, public gross fixed capital formation (i.e., public investment) and government social assistance payments (i.e., social subsidies) are the explanatory variables of interest. Government final consumption is included to control for the effects of less productive, more routine government spending. Lastly, non-tax revenue (which, arguably, have a less distortionary effect on markets compared to taxes) and monetary policy reflected through the cash rate target and M3 money supply are also controlled for to mitigate omitted variable issues.

Outcome variables:

-   Unemployment rate and nominal GDP

Explanatory variables of interest

-   Revenue: Tax revenue

-   Spending: Public gross fixed capital formation and social assistance payments

Control variables:

-   Other fiscal policies: non-tax (gross income less tax) revenue, government final consumption
-   Monetary policy: cash rate target and M3 money supply
-   External sector: real exchange rate

```{r set up and libraries, include = F}
rm(list=ls())
library(readrba)
library(readabs)
library(xts)
library(tseries)
library(urca)
library(FinTS)
library(rmarkdown)
library(corrplot)
library(parallel)
library(MASS)
library(coda)
library(sads)
library(truncnorm)
library(mvtnorm)
library(plot3D)
library(HDInterval)
library("RColorBrewer")
```

```{r data download, include = F}
#| message: false
#rba <- browse_rba_series(search_string = "")
#abs <- as.matrix(show_available_catalogues())
#remotes::install_github("mattcowgill/readrba")

## Monetary policy variables
# Cash interest rate
cash_rate.dl   <- read_rba(series_id = "FIRMMCRTD")
cashrate <- to.quarterly(xts(cash_rate.dl$value, cash_rate.dl$date), 
                         OHLC = FALSE)
# M3 Money supply
M3.dl   <- read_rba(series_id = "DMAM3S")
M3 <- to.quarterly(xts(M3.dl$value, M3.dl$date), 
                         OHLC = FALSE)

# real exchange rate (TWI)
# realTWI.dl   <- read_rba(series_id = "FRERTWI")
# realTWI <- to.quarterly(xts(realTWI.dl$value, realTWI.dl$date), 
                         # OHLC = FALSE)

# Unemployment rate: A84423092X 
# sadj A84423050A
unemp_rate.dl <- read_abs(series_id = "A84423050A")
unemp <- to.quarterly(xts(unemp_rate.dl$value, unemp_rate.dl$date), 
                           OHLC = FALSE)

# real GDP: A2302459A
# sadj A2304402X
realGDP.dl   <- read_abs(series_id = "A2304402X")
realGDP <- to.quarterly(xts(realGDP.dl$value, realGDP.dl$date), 
                         OHLC = FALSE)

# Nominal GDP: A2302467A
# nomGDP.dl <- read_abs(series_id = "A2302467A")
# nomGDP <- to.quarterly(xts(nomGDP.dl$value, nomGDP.dl$date), 
#                          OHLC = FALSE)

#summary(unemp_rate)
## Fiscal variables
# Total tax: A2301963V 
totaltax.dl <- read_abs(series_id = "A2302794K")
totaltax <- to.quarterly(xts(totaltax.dl$value, totaltax.dl$date), 
                         OHLC = FALSE)

# Non-tax revenue: Gross income (A2302106V) - total tax 
govgrossinc.dl <- read_abs(series_id = "A2302742J")
govgrossinc <- to.quarterly(xts(govgrossinc.dl$value, govgrossinc.dl$date), 
                         OHLC = FALSE)

nontax <- govgrossinc - totaltax

# Public gross fixed capital formation: A2302555A
pubinv.dl <- read_abs(series_id = "A2304065W")
pubinv <- to.quarterly(xts(pubinv.dl$value, pubinv.dl$date), 
                         OHLC = FALSE)

# Social assistance benefits payments: A2301976F
pubtrans.dl <- read_abs(series_id = "A2301976F")
pubtrans <- to.quarterly(xts(pubtrans.dl$value, pubtrans.dl$date), 
                         OHLC = FALSE)

# Government final consumption: A2302527T
pubcons.dl <- read_abs(series_id = "A2304036K")
pubcons <- to.quarterly(xts(pubcons.dl$value, pubcons.dl$date),
                         OHLC = FALSE)

```

##### Table 1. Data from ABS and RBA {style="text-align: center;"}

```{r data merge and log transform, echo = F}
# Merge data into one matrix
Y.df  <- na.omit(merge(unemp, realGDP , totaltax, nontax, pubinv, 
                       pubtrans, pubcons, cashrate, M3))

varname_vec <- c("Unemployment rate", "Real GDP", "Tax revenue", "Non-tax revenue", "Gov't GFCF",
                    "Social benefits payments", "Gov't consumption", "Cash rate target", "M3 supply")
colnames(Y.df) <- varname_vec

Y.df <- Y.df[1:132,]
# Transform into natural logs
lnY.df <- log(Y.df)

date <- as.vector(index(cashrate))[1:132]
T <- length(date)

paged_table(as.data.frame(round(head(lnY.df,4),2)))
paged_table(as.data.frame(round(tail(lnY.df,4),2)))

# data for input to estimation function 
lnY.df_num <- c()

for (i in 1:ncol(lnY.df)){
  lnY.df_num <- cbind(lnY.df_num, as.numeric(lnY.df[,i]))
}

```

# 

As seen in the figures below, Australian GDP output has been on a relatively steady, upward trend since 1990, along with fiscal revenue and spending. These have, by and large, been accommodated by declining interest rates and an expanding money supply. Overall, these have been associated with a downward, albeit volatile trajectory in the unemployment rate. Currently, the data is left in original, seasonally-unadjusted form as the variable for social benefit payments does not have a seasonally-adjusted version in the ABS website. As such, some seasonal variation between quarters can be observed in time series plots. However, the final version of the data to be used will be determined after further exploration. Lastly, the data will be transformed into natural log terms during the analysis as to interpret the coefficients in the structural matrix as elasticities.

##### Figure 1. Time series plots: 1990 Q1 to 2022 Q4, original values {style="text-align: center;"}

```{r, echo = F}
colvec <- c("turquoise4","turquoise4", "plum4", "plum4", "plum4", "plum4", "plum4","firebrick3","firebrick3")
par(mfrow=c(3,3), mar=c(2,2,2,2))
for (i in 1:9){
  plot(date, y = Y.df[,i], type = "l", 
       main = paste(varname_vec[i]), ylab = "", xlab = "",
       col = colvec[i], lwd = 1.5,
       ylim = c(min(Y.df[,i]),max(Y.df[,i])))
}

```

## Preliminary data analysis

### ACF and PACF analysis

Across variables (which have been transformed into natural log terms), ACF plots show a strong, positive, and gradually decaying autocorrelation structure. Moreover various PACF plots indicate a near-one value at the first lag and much lower, mostly statistically zero values at higher lag orders (such as in nominal GDP, social benefits payments, government consumption, and M3). These characteristics may indicate that some variables may be following a random walk with drift process and as such may be unit-root non-stationary.

###### Figure 2.1. Autocorrelation function (ACF) plots of variables in log terms {style="text-align: center;"}

```{r ACF plots, echo = FALSE}
par(mfrow=c(3,3), mar=c(2,2,2,2))
for (i in 1:9){
  Acf(lnY.df[,i], main = "", ylim = c(-0.2,1))
  title(main = paste(varname_vec[i]), line = 0.5)
} 
```

###### Figure 2.2. Partial autocorrelation function (PACF) plots of variables in log terms {style="text-align: center;"}

```{r PACF plotsPACF analysis, echo = FALSE}
par(mfrow=c(3,3), mar=c(2,2,2,2))
for (i in 1:9){
  pacf(lnY.df[,i], main = "", ylim = c(-0.2,1))
  title(main = paste(varname_vec[i]), line = 0.5)
} 

```

### Augmented Dickey-Fuller test for unit roots

```{r Ng-Perron ADF test function, echo = TRUE}
# function to implement the Ng and Perron (1998) ADF test procedure
ngperronADF <- function(y){
  T <- length(y)
  kmax <- ceiling(12*(((T-25)/100)^(1/4)))
  k <- kmax + 1
  # run test regression until t-statistic of kth lag > 1.6
  t <- 0
  while(abs(t) < 1.6){
      k <- k - 1
      adft <- ur.df(y, type = "drift", lags = k)
      t <- as.numeric(adft@testreg$coefficients[k+2,][3])
  }
  adft_k <-ur.df(y, type = "drift", lags = k)
  teststat <- adft_k@teststat[2]
  cval <- adft_k@cval[2,2]
  sig <- abs(teststat)>abs(cval)
  return(list("stat" = teststat,"crit" = cval,"reject" = sig))
}

```

##### Table 2. ADF test results: levels, first, and second difference {style="text-align: center;"}

```{r ADF test and order of integration determination, echo = FALSE}

adf_results_lev <- c()
adf_results_fd <- c()
adf_results_fd2 <- c()

for (i in 1:9){
npadf_lev <- ngperronADF(lnY.df[,i])  
npadf_fd <- ngperronADF(diff(lnY.df[,i])[-1]) 
npadf_fd2 <- ngperronADF(diff(diff(lnY.df[,i])[-1])[-1]) 


adf_results_lev <- round(rbind(adf_results_lev,c(npadf_lev[[1]],
                                           npadf_lev[[3]])),2)

adf_results_fd <- round(rbind(adf_results_fd,c(npadf_fd[[1]],
                                         npadf_fd[[3]])),2)
adf_results_fd2 <- round(rbind(adf_results_fd2,c(npadf_fd2[[1]],
                                        npadf_fd2[[3]])),2)
}

colnames(adf_results_lev) <- c("ADF statistic, levels", "Reject?")
colnames(adf_results_fd) <- c("ADF statistic, 1st difference", "Reject?")
colnames(adf_results_fd2) <- c("ADF statistic, 2nd difference", "Reject?")

rownames(adf_results_lev) <- varname_vec
rownames(adf_results_fd) <- varname_vec
rownames(adf_results_fd2) <- varname_vec

adf_results <- cbind(adf_results_lev,adf_results_fd, adf_results_fd2)

paged_table(as.data.frame(adf_results))


for (i in 1:9){
npadf_lev <- ngperronADF(lnY.df[,i])  
#npadf_fd <- ngperronADF(diff(lnY.df[,i])[-1]p)  
}
```

# 

Augmented Dickey-Fuller tests following the maximum lag-setting procedure of Ng and Perron (1998) were performed on all nine variables to determine stationarity and orders of integration. The null hypothesis that a unit roots exists was rejected for ADF tests on levels of tax revenue, non-tax revenue, social benefits payments, government consumption, and M3 supply, indicating that these variables do not have a unit-root, and may be following trend-stationary autoregressive processes. Meanwhile, the first differences of the unemployment rate, government GFCF, and cash rate target variables yielded a significant ADF test result, indicating that these have order of integration of 1. Lastly, it took another differencing for nominal GDP to yield a significant result, indicating an order of integration of 2. Further analysis of these variables, including a better-calibrated ADF test and cointegration tests will be performed in the future.

## Model specification

The use of fiscal policy to stimulate or regulate the economy follows the Keynesian approach generally practiced around the world (but especially in developing countries) to navigate economies through downturns or potential episodes of overheating, particularly in the short-run. The proposed model aims to investigate the effectiveness of these fiscal measures in managing growth and unemployment.

### Structural form (SF) model

The following SVAR model specification is proposed is to represent the system through which the included variables are jointly determined:

$$
\begin{align}
B_0y_t &= b_0 + B_1 y_{t-1} + \dots + B_p y_{t-p} + u_t\\
u_{t}| Y_{t-1} &\sim _{iid} ( 0, I_N)
\end{align}
$$ where $y_t$ is a vector of endogenous variables:

$$y_t=\begin{pmatrix} unemp_t &= \text{unemployment rate}
\\ realgdp_t &= \text{real GDP}
\\ totaltax_t  &= \text{tax revenue}
\\ nontax_t  &= \text{non-tax revenue}
\\ pubinv_t  &= \text{government gross fixed capital formation}
\\ pubtrans_t  &= \text{social assitance and benefits payments}
\\ pubcons_t  &= \text{government final consumption}
\\ cashrate_t  &= \text{cash rate target}
\\ M3_t  &= \text{M3 money supply}
\end{pmatrix}$$

and the structural matrix $B_0$ summarizes the contemporaneous relationships between these variables.

### Reduced form (RF) model

$$
\begin{align}
y_t &= \mu_0 + A_1 y_{t-1} + \dots + A_p y_{t-p} + \varepsilon_t\\
\text{where }B_0^{-1}u_t &= \varepsilon_t| Y_{t-1} \sim _{iid} ( 0, \Sigma)\\
\Sigma &= B_0^{-1}B_0^{-1'}
\end{align}
$$

The reduced form of the model, where $B_0^{-1}$ is the matrix of contemporaneous effects between variables, allows for model estimation. However, the identification strategy for $B_0^{-1}$ is yet to be finalized, but will likely involve either exclusion restrictions or sign restrictions.

The study will utilize impulse response functions (IRF) and forecast error variance decomposition (FEVD) methods to measure the effects of the tax policy, public investment, and government social transfers on the unemployment rate and GDP output. IRFs are used to measure the dynamic, marginal effects of orthogonal shocks from the three fiscal levers of interest while FEVD intends to account for the share of explained variability in the outcome variables that can be attributed to explanatory variables of interest.



```{r Basic model estimation function}
# data = input data should be quarterly
# p = lags
# S = number of posterior draws
# sign restritions = Nx1 diagonal of R matrix
# k1 = kappa1
# k2 = kappa2, higher value, less shrinkage, more weight on prior
# start date = start date of Y matrix
sign.basic <- function(data, p, S,  sign.restrictions,
                           k1=0.04, k2=100, start_date = c(1991,1), shockvar){
  # Define Y and X matrices
  ############################################################
  # N = no. of variables
  N = ncol(data)
  # p = no. of lags
  K = 1 + p*N
  # forecast horizon
  # h       = 8
  
  Y       = ts(data[(p+1):nrow(data),], start=start_date, frequency=4)
  X       = matrix(1,nrow(Y),1)
  # nrow(X)
  for (i in 1:p){
    X     = cbind(X,data[(p+1):nrow(data)-i,])
  }
  
  #t0          = proc.time() # read processor time
  
  # Calculate MLE
  ############################################################
  A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
  Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)
  # round(A.hat,3)
  # round(Sigma.hat,3)
  # round(cov2cor(Sigma.hat),3)
  
  # Specify prior distribution
  ############################################################
  kappa.1     = k1
  kappa.2     = k2
  kappa.3     = 1
  A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))
  A.prior[2:(N+1),] = kappa.3*diag(N)
  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
  S.prior     = diag(diag(Sigma.hat))
  nu.prior    = N+1
  
  # matrix normal-inverse Wishart posterior parameters
  ############################################################
  V.bar.inv   = t(X)%*%X + diag(1/diag(V.prior))
  V.bar       = solve(V.bar.inv)
  A.bar       = V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)
  nu.bar      = nrow(Y) + nu.prior
  S.bar       = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
  S.bar.inv   = solve(S.bar)
  
  # posterior draws 
  ############################################################
  ## draw from RF posterior
  # draw Sigma from inverse wishart
  Sigma.posterior   = rWishart(S, df=nu.bar, Sigma=S.bar.inv)
  Sigma.posterior   = apply(Sigma.posterior,3,solve)
  Sigma.posterior   = array(Sigma.posterior,c(N,N,S))
  
  # draw A from matrix-variate normal
  A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S))
  
  ## draw from SF posterior
  B0.posterior       = array(NA,c(N,N,S))
  Bplus.posterior       = array(NA,c(N,K,S))
  L                 = t(chol(V.bar))
  for (s in 1:S){
    # Draw B0
    cholSigma.s     = chol(Sigma.posterior[,,s])
    B0.posterior[,,s]= solve(t(cholSigma.s))
    A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%cholSigma.s
    # Draw Bplus
    Bplus.posterior[,,s] = B0.posterior[,,s]%*%t(A.posterior[,,s])
  }
  # round(apply(A.posterior,1:2,mean),4)
  # round(apply(B.posterior,1:2,mean),4)
  
  # Identification via sign restrictions on theta0
  ############################################################
  # should have N elements
  #sign.restrictions = c()
  
  # generate corresponding R matrix
  R1 = diag(sign.restrictions)
  
  # storage matrices for Q identified estimates
  i.vec <- c()
  Q.iden   = array(NA,c(N,N,S))
  B0.iden = array(NA,c(N,N,S))
  B1.iden = array(NA,c(N,K,S))
  A.iden = array (NA,c(K,N,S))
  
  pb = txtProgressBar(min = 0, max = S, initial = 0) 

  for (s in 1:S){
    
    setTxtProgressBar(pb,s)
    #cat(". iteration: ", s, "\n")
    
    # pick-up a B0 from S
    B0.tilde <- B0.posterior[,,s]
    IR.0.tilde    = solve(B0.tilde)
    B1.tilde      = Bplus.posterior[,,s]
    #IR.1.tilde    = solve(B0.tilde)%*%B1.tilde%*%solve(B0.tilde)

    # Search for appropriate Q 
    sign.restrictions.do.not.hold = TRUE
    i=1
    while (sign.restrictions.do.not.hold){
      X           = matrix(rnorm(N^2),N,N)
      QR          = qr(X, tol = 1e-10)
      Q           = qr.Q(QR,complete=TRUE)
      R           = qr.R(QR,complete=TRUE)
      Q           = t(Q %*% diag(sign(diag(R))))
      B0          = Q%*%B0.tilde
      B1          = Q%*%B1.tilde
      B0.inv      = solve(B0)
      check       = prod(R1 %*% B0.inv %*% diag(N)[,shockvar] >= 0)
      A           = t(solve(B0)%*%B1)

      if (check==1){sign.restrictions.do.not.hold=FALSE}
      i=i+1
    }
    i.vec <- c(i.vec, i)
    Q.iden[,,s] <- Q
    B0.iden[,,s] <- B0
    B0.mean <- apply(B0.iden,1:2,mean)
    B1.iden[,,s] <- B1
    B1.mean <- apply(B1.iden,1:2,mean)
    A.iden[,,s] <- A
    A.mean <- apply(A.iden,1:2,mean)
    

  }
  re <- list("i" = i.vec, "Q" = Q.iden, "B0"= B0.iden, "B0.mean" = B0.mean,
             "Bplus"= B1.iden, "Bplus.mean" = B1.mean, "A" = A.iden, "A.mean" = A,
             "A.posterior"=A.posterior, "Sigma.posterior"=Sigma.posterior)
  return(re)
}

```

```{r RW sample}
set.seed(1)
y1 <- arima.sim(model= list(order = c(0, 1, 0)), n=1000, mean=1,sd=2)
y2 <- arima.sim(model= list(order = c(0, 1, 0)), n=1000, mean=1,sd=2)
# y1 <- cumsum(rnorm(1000, 0, sd=1))
# y2 <- cumsum(rnorm(1000, 0, sd=1)) 
y <- cbind(y1,y2)

par(mfrow=c(1,1))
plot.ts(y)
#plot(y[,3], type = "l", col = "darkorchid3")



RW_res <-  sign.basic(data=y, p=1, S=1000, c(1,-1), k1 = 0.04, k2 = 100, shockvar = 1)
round(RW_res$A.mean,1)

```


```{r Basic model IRF and FEVD}

```


```{r Posterior maximizing function}

v.posterior.mode <- function(data, p=4, k1=1, k2=100, start_date=c(1991,1)){

  v.neglogPost <- function(theta){
    N = ncol(data)
    # p = no. of lags
    K = 1 + p*N
    # forecast horizon
    # h       = 8
    Y       = ts(data[(p+1):nrow(data),], start=start_date, frequency=4)
    T = nrow(Y)
    X       = matrix(1,T,1)
    # nrow(X)
    for (i in 1:p){
      X     = cbind(X,data[(p+1):nrow(data)-i,])
    }
    
      # Calculate MLE for prior 
    ############################################################
    A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
    Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)
    # round(A.hat,3)
    # round(Sigma.hat,3)
    # round(cov2cor(Sigma.hat),3)
    
    # Specify prior distribution
    ############################################################
    kappa.1     = k1
    kappa.2     = k2
    kappa.3     = 1
    A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))
    A.prior[2:(N+1),] = kappa.3*diag(N)
    V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
    S.prior     = diag(diag(Sigma.hat))
    nu.prior    = N+1
    
    vec <- theta[1:3]
    for (i in 4:12){
      vec <- c(vec, 1 + (theta[3]-1)*theta[4]^(i-3))
    }  
    
    V <- c(ts(rep(1, nrow(Y)-12), c(1991,1), frequency = 4) , vec)    
    
    Y.tilde <- diag(1/V)%*%Y
    X.tilde <- diag(1/V)%*%X
    A.tilde.hat <- solve((t(X.tilde)%*%X.tilde+solve(V.prior)))%*%(t(X.tilde)%*%Y.tilde+solve(V.prior)%*%A.prior)
    epsilon.tilde <-Y.tilde - X.tilde%*%A.tilde.hat
        
    logL <- log(prod(V^(-N)))+(-N/2)*log(det(t(X.tilde)%*%X.tilde+solve(V.prior)))+
            (-(T-p+nu.prior)/2)*log(det(S.prior +t(epsilon.tilde)%*%epsilon.tilde + 
            t(A.tilde.hat-A.prior)%*%solve(V.prior)%*%(A.tilde.hat-A.prior)))

    # "Computationally stable" equivalent
    # logL <- log(prod(V^(-N)))+(-N/2)*log(det(t(chol(V.prior))%*%t(X.tilde)%*%X.tilde%*%chol(V.prior)+diag(K)))+
    #           (-(T-p+nu.prior)/2)*log(det(diag(N) + t(chol(S.prior))%*%(t(epsilon.tilde)%*%epsilon.tilde +
    #           t(A.tilde.hat-A.prior)%*%solve(V.prior)%*%(A.tilde.hat-A.prior))%*%chol(S.prior)))      
    #           

    pareto.a=1
    pareto.b=1
    beta.a=3
    beta.b=1.5
    beta.cons <- 1/beta(beta.a,beta.b)
    
    logP <- log((pareto.a*pareto.b^pareto.a)/(theta[1]^(pareto.a+1))*
    (pareto.a*pareto.b^pareto.a)/(theta[2]^(pareto.a+1))*
    (pareto.a*pareto.b^pareto.a)/(theta[3]^(pareto.a+1))*
    beta.cons*theta[4]^(beta.a-1)*(1-theta[4])^(beta.b-1))
    
    neglogPost <- -(logL+logP)
    
    return(neglogPost)
  }
   
  
  post.maximizer <- optim(par=c(50, 50, 50, 0.5), fn=v.neglogPost, method="L-BFGS-B", 
                          lower=c(1, 1, 1, 0.0001),
                          upper=c(100,100,100,0.99999), hessian = TRUE)
  
  return(list(maximizer=post.maximizer$par, hessian=post.maximizer$hessian))

}


print(v.posterior.mode(y, p = 1))
print(v.posterior.mode(lnY.df_num, p = 4))

```


```{r Metropolis-Hastings function}
mh.mcmc <- function(data, p=1, S.mh = 1000, c, W = diag(4), theta.init,
                    k1 = 1, k2 = 100, start_date = c(1991,1)){
 # N = no. of variables
  N = ncol(data)
  # p = no. of lags
  K = 1 + p*N
  # forecast horizon
  # h       = 8
  Y       = ts(data[(p+1):nrow(data),], start=start_date, frequency=4)
  T = nrow(Y)
  X       = matrix(1,T,1)
  # nrow(X)
  for (i in 1:p){
    X     = cbind(X,data[(p+1):nrow(data)-i,])
  }
  

  # Calculate MLE for prior 
  ############################################################
  A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
  Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)
  # round(A.hat,3)
  # round(Sigma.hat,3)
  # round(cov2cor(Sigma.hat),3)
  
  # Specify prior distribution
  ############################################################
  kappa.1     = k1
  kappa.2     = k2
  kappa.3     = 1
  A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))
  A.prior[2:(N+1),] = kappa.3*diag(N)
  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
  S.prior     = diag(diag(Sigma.hat))
  nu.prior    = N+1
  
  # Metropolis-Hastings 
  ###########################################################
  # v0, v1, v2, rho
  Theta <- matrix(NA,S.mh,4)
  theta_old <- theta.init
  #theta_old <- Theta[nrow(Theta),]
  
  # W <- diag(4)
  set.seed(1)
  pb = txtProgressBar(min = 0, max = S.mh, initial = 0) 
  for (s in 1:S.mh){
    setTxtProgressBar(pb,s)

    covid.vec <- function(theta){
      vec <- theta[1:3]
      for (i in 4:12){
        vec <- c(vec, 1 + (theta[3]-1)*theta[4]^(i-3))
      }
      
      return(vec)
    }

    # Covid volatility likelihood kernel
    #v.lik <- function(V, N) det(diag(V^2))^(-N/2)
    v.logL <- function(V){
      Y.tilde <- diag(1/V)%*%Y
      X.tilde <- diag(1/V)%*%X
      A.tilde.hat <- solve((t(X.tilde)%*%X.tilde+solve(V.prior)))%*%(t(X.tilde)%*%Y.tilde+solve(V.prior)%*%A.prior)
      epsilon.tilde <-Y.tilde - X.tilde%*%A.tilde.hat

      logL <- log(prod(V^(-N)))+(-N/2)*log(det(t(X.tilde)%*%X.tilde+solve(V.prior)))+
              (-(T-p+nu.prior)/2)*log(det(S.prior +t(epsilon.tilde)%*%epsilon.tilde + 
              t(A.tilde.hat-A.prior)%*%solve(V.prior)%*%(A.tilde.hat-A.prior)))

      # Computationally stable equivalent
      # logL <- log(prod(V^(-N)))+
      #         (-N/2)*log(det(t(chol(V.prior))%*%t(X.tilde)%*%X.tilde%*%chol(V.prior)+diag(K)))+
      #         (-(T-p+nu.prior)/2)*
      #   log(det(diag(N) + t(chol(S.prior))%*%(t(epsilon.tilde)%*%epsilon.tilde +
      #                           t(A.tilde.hat-A.prior)%*%solve(V.prior)%*%(A.tilde.hat-A.prior))%*%chol(S.prior)))      
      #         
      return(logL)
    }
  
    # Covid volatility prior
    v.logP <- function(theta, pareto.a=1, pareto.b=1, beta.a=3, beta.b=1.5){
      beta.cons <- 1/beta(beta.a,beta.b)
  
      logP <- log((pareto.a*pareto.b^pareto.a)/(theta[1]^(pareto.a+1))*
      (pareto.a*pareto.b^pareto.a)/(theta[2]^(pareto.a+1))*
      (pareto.a*pareto.b^pareto.a)/(theta[3]^(pareto.a+1))*
       beta.cons*theta[4]^(beta.a-1)*(1-theta[4])^(beta.b-1))
      #dnorm(theta[4],0,1)
      
      return(logP)
    }

    v_ones <- ts(rep(1, nrow(Y)-12), c(1991,1), frequency = 4) 
    V.old <- c(v_ones, covid.vec(theta_old))    
      
    # New candidate parameters values
    theta_new <- mvrnorm(1, theta_old, c*W)
    V.new <- c(v_ones, covid.vec(theta_new))
    
    # calculate posteriors 
    v.logpost_old <- v.logL(V.old)+v.logP(theta_old)
    v.logpost_new <- v.logL(V.new)+v.logP(theta_new)
    
    post.ratio <- exp(v.logpost_new-v.logpost_old)
  
    alpha <- min(1, post.ratio)
    
    u_star <- runif(1)
    
    if (alpha > u_star){
      Theta[s,] <- theta_new
    } else {Theta[s,] <- theta_old}
    
    theta_old <- Theta[s,]  
  }
  
  colnames(Theta) <- c("v0", "v1" , "v2", "rho")

  #plot.ts(Theta)
  #1 - rejectionRate(as.mcmc(Theta[,1]))
  #Theta[nrow(Theta),]
  re <- list(Theta=Theta, 
             AcceptRate = 1 - rejectionRate(as.mcmc(Theta[,1])))
  return(re)
}

```

```{r RW MCMC runs}
RM.posterior.mode <- v.posterior.mode(y, p = 1)$maximizer
RM.posterior.Hessian <- v.posterior.mode(y, p = 1)$hessian

set.seed(1)
RM.theta <- mh.mcmc(y, c = 0.00001, W = diag(4), theta.init = RM.posterior.mode, S.mh = 10000)
plot.ts(mh1$Theta)
mh1$AcceptRate


```

```{r Data MCMC runs}

theta.ext.post.mode <- v.posterior.mode(lnY.df_num, p = 4, k1 = 1, k2 = 100)$maximizer
theta.ext.post.Hessian <- v.posterior.mode(lnY.df_num, p = 4, k1 = 1, k2 = 100)$hessian


set.seed(1)
mhx <-mh.mcmc(lnY.df_num, p=4, c=0.000228, W = solve(theta.ext.post.Hessian), 
                theta.init = theta.ext.post.mode, k1 = 1, S.mh = 20000)
plot.ts(mhx$Theta)

set.seed(1)
mhx.2 <-mh.mcmc(lnY.df_num, p=4, c = 0.001, W = cov(mhx$Theta), theta.init = mhx$Theta[nrow(mhx$Theta),], 
                k1 = 1, S.mh = 10000)
plot.ts(mhx.2$Theta)
mhx.2$AcceptRate

set.seed(1)
mhx.3 <-mh.mcmc(lnY.df_num, p=4, c = 0.0025, W = cov(mhx.2$Theta), theta.init = mhx.2$Theta[nrow(mhx.2$Theta),], k1=1,S.mh = 10000)
plot.ts(mhx.3$Theta)
mhx.3$AcceptRate


set.seed(1)
mhx.4 <-mh.mcmc(lnY.df_num, p=4, c = 0.001, W = cov(mhx.3$Theta), theta.init = mhx.3$Theta[nrow(mhx.3$Theta),], k1=1, S.mh = 10000)
plot.ts(mhx.4$Theta)
mhx.4$AcceptRate

```


```{r Extended model}
sign.extension <- function(data, p=4, S=100,  sign.restrictions = c(0, 0, 1, 1, 1, -1, 1, -1, 1),
                           k1=1, k2=100, shockvar = 5, start_date = c(1991,1), Theta.mh){

  # N = no. of variables
  N = ncol(data)
  # p = no. of lags
  K = 1 + p*N
  # forecast horizon
  # h       = 8
  Y       = ts(data[(p+1):nrow(data),], start=start_date, frequency=4)
  T = nrow(Y)
  X       = matrix(1,T,1)
  # nrow(X)
  for (i in 1:p){
    X     = cbind(X,data[(p+1):nrow(data)-i,])
  }
  
  
  covid.vec <- function(theta){
    vec <- theta[1:3]
    for (i in 4:12){
      vec <- c(vec, 1 + (theta[3]-1)*theta[4]^(i-3))
    }
      
    return(vec)
  }
  

  # array of S diag(covid volatility) matrices
  diagV.sqinv <- array(NA, c(nrow(Y),nrow(Y),S))
  
  for (s in 1:S){
    v_ones <- ts(rep(1, nrow(Y)-12), c(1991,1), frequency = 4) 
    diagV.sqinv[,,s] <- diag(c(v_ones, covid.vec(Theta.mh[s,]))^(-2))
  }
  
  # Calculate MLE for prior 
  ############################################################
  A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
  Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)
  # round(A.hat,3)
  # round(Sigma.hat,3)
  # round(cov2cor(Sigma.hat),3)
  
  # Specify prior distribution
  ############################################################
  kappa.1     = k1
  kappa.2     = k2
  kappa.3     = 1
  A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))
  A.prior[2:(N+1),] = kappa.3*diag(N)
  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
  S.prior     = diag(diag(Sigma.hat))
  nu.prior    = N+1
  
  # Posterior draws 
  ############################################################
  Sigma.posterior   = array(NA,c(N,N,S))
  A.posterior       = array (NA,c(K,N,S))
  B0.posterior       = array(NA,c(N,N,S))
  Bplus.posterior       = array(NA,c(N,K,S))
  
  pb = txtProgressBar(min = 0, max = S, initial = 0) 
  for (s in 1:S){
    setTxtProgressBar(pb,s)
    V.bar.inv   = t(X)%*%diagV.sqinv[,,s]%*%X + diag(1/diag(V.prior))
    V.bar       = solve(V.bar.inv)
    A.bar       = V.bar%*%(t(X)%*%diagV.sqinv[,,s]%*%Y + diag(1/diag(V.prior))%*%A.prior)
    nu.bar      = nrow(Y) + nu.prior
    S.bar       = S.prior + t(Y)%*%diagV.sqinv[,,s]%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%
                  A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
    S.bar.inv   = solve(S.bar)
    L                 = t(chol(V.bar))
    
    # RF posterior draws
    Sigma.posterior[,,s] <- solve(rWishart(1, df=nu.bar, Sigma=S.bar.inv)[,,1])
    cholSigma.s     = chol(Sigma.posterior[,,s])
    A.posterior[,,s]       = matrix(mvrnorm(1,as.vector(A.bar), Sigma.posterior[,,s]%x%V.bar),ncol=N)
    A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%cholSigma.s
    
    # SF posterior draws 
    B0.posterior[,,s]= solve(t(cholSigma.s))
    # Draw Bplus
    Bplus.posterior[,,s] = B0.posterior[,,s]%*%t(A.posterior[,,s])
  }

  # Identification via sign restrictions on theta0
  ############################################################

  # generate corresponding R matrix
  R1 = diag(sign.restrictions)
  
  # storage matrices for Q identified estimates
  i.vec <- c()
  Q.iden   = array(NA,c(N,N,S))
  B0.iden = array(NA,c(N,N,S))
  B1.iden = array(NA,c(N,K,S))
  A.iden = array (NA,c(K,N,S))
  
  pb = txtProgressBar(min = 0, max = S, initial = 0) 

  for (s in 1:S){
    
    setTxtProgressBar(pb,s)
    #cat(". iteration: ", s, "\n")
    
    # pick-up a B0 from S
    B0.tilde <- B0.posterior[,,s]
    IR.0.tilde    = solve(B0.tilde)
    B1.tilde      = Bplus.posterior[,,s]
    #IR.1.tilde    = solve(B0.tilde)%*%B1.tilde%*%solve(B0.tilde)

    # Search for appropriate Q 
    sign.restrictions.do.not.hold = TRUE
    i=1
    while (sign.restrictions.do.not.hold){
      X           = matrix(rnorm(N^2),N,N)
      QR          = qr(X, tol = 1e-10)
      Q           = qr.Q(QR,complete=TRUE)
      R           = qr.R(QR,complete=TRUE)
      Q           = t(Q %*% diag(sign(diag(R))))
      B0          = Q%*%B0.tilde
      B1          = Q%*%B1.tilde
      B0.inv      = solve(B0)
      check       = prod(R1 %*% B0.inv %*% diag(N)[,shockvar] >= 0)
      A           = t(solve(B0)%*%B1)

      if (check==1){sign.restrictions.do.not.hold=FALSE}
      i=i+1
    }
    i.vec <- c(i.vec, i)
    Q.iden[,,s] <- Q
    B0.iden[,,s] <- B0
    B0.mean <- apply(B0.iden,1:2,mean)
    B1.iden[,,s] <- B1
    B1.mean <- apply(B1.iden,1:2,mean)
    A.iden[,,s] <- A
    A.mean <- apply(A.iden,1:2,mean)
  }

  re <- list("i" = i.vec, "Q" = Q.iden, "B0"= B0.iden, "B0.mean" = B0.mean,
             "Bplus"= B1.iden, "Bplus.mean" = B1.mean, "A" = A.iden, "A.mean" = A,
             "A.posterior"=A.posterior, "Sigma.posterior"=Sigma.posterior, "Theta"= Theta.mh)
  return(re)
  
}

```

```{r Basic model plots}
#expansionary fiscal policy, accommodated by monetary policy
basic.model <- sign.basic(lnY.df_num,p=4,S=10000,c(0, 0, 1, 1, 1, 1, 1, -1, 1), k1 = 1, shockvar=5)

N <- dim(basic.model$A.posterior)[2]
S <- dim(basic.model$A.posterior)[3]
p <- 4
h <- 12

A.posterior.basic <- basic.model$A.posterior

B0.posterior.basic <- basic.model$B0

B.posterior.basic <- array(NA, c(N,N,S))
for (s in 1:S){
  B.posterior.basic[,,s] <- solve(B0.posterior.basic[,,s])
}

# Define colors
mcxs1  = "#05386B"
mcxs2  = "#379683"
mcxs3  = "#5CDB95"
mcxs4  = "#8EE4AF"
mcxs5  = "#EDF5E1"
purple = "#b02442"

mcxs1.rgb   = col2rgb(mcxs1)
mcxs1.shade1= rgb(mcxs1.rgb[1],mcxs1.rgb[2],mcxs1.rgb[3], alpha=120, maxColorValue=255)
mcxs2.rgb   = col2rgb(mcxs2)
mcxs2.shade1= rgb(mcxs2.rgb[1],mcxs2.rgb[2],mcxs2.rgb[3], alpha=120, maxColorValue=255)


# Impulse response functions
# Forecast Error Variance Decomposition
############################################################
A.posterior <- A.posterior.basic
B.posterior <- B.posterior.basic

IRF.posterior     = array(NA,c(N,N,h+1,S))
IRF.inf.posterior = array(NA,c(N,N,S))
FEVD.posterior    = array(NA,c(N,N,h+1,S))
J                 = cbind(diag(N),matrix(0,N,N*(p-1)))

pb = txtProgressBar(min = 0, max = S, initial = 0) 
for (s in 1:S){
  setTxtProgressBar(pb,s)
  A.bold          = rbind(t(A.posterior[2:(1+N*p),,s]),cbind(diag(N*(p-1)),matrix(0,N*(p-1),N)))
  IRF.inf.posterior[,,s]          = J %*% solve(diag(N*p)-A.bold) %*% t(J) %*% B.posterior[,,s]
  A.bold.power    = A.bold
  for (i in 1:(h+1)){
    if (i==1){
      IRF.posterior[,,i,s]        = B.posterior[,,s]
    } else {
      IRF.posterior[,,i,s]        = J %*% A.bold.power %*% t(J) %*% B.posterior[,,s]
      A.bold.power                = A.bold.power %*% A.bold
    }
    for (n in 1:N){
      for (nn in 1:N){
        FEVD.posterior[n,nn,i,s]  = sum(IRF.posterior[n,nn,1:i,s]^2)
      }
    }
    FEVD.posterior[,,i,s]         = diag(1/apply(FEVD.posterior[,,i,s],1,sum))%*%FEVD.posterior[,,i,s]
  }
}
FEVD.posterior    = 100*FEVD.posterior

#t1          = proc.time()
#(t1-t0)[3]/60 # Time of computations in minutes

# save(IRF.posterior,IRF.inf.posterior, FEVD.posterior, file="irf-fevd-k002.RData")
save(IRF.posterior,IRF.inf.posterior, FEVD.posterior, file="irf-fevd-k1.RData")


load("irf-fevd-k1.RData")
# IRF plots GFCF
############################################################

shock.var <- 5
IRFs.k1           = apply(IRF.posterior[1:9,shock.var,,],1:2,mean)
IRFs.inf.k1       = apply(IRF.inf.posterior[1:9,shock.var,],1,mean)
rownames(IRFs.k1) = varname_vec

IRFs.k1.hdi    = apply(IRF.posterior[1:9,shock.var,,],1:2,hdi, credMass=0.68)
hh          = 1:(h+1)

#pdf(file="irf-GFCF.pdf", height=9, width=12)
par(mfrow=c(3,3), mar=c(3,3,2,2),cex.axis=1.5, cex.lab=1.5)
for (n in 1:N){
  ylims     = range(IRFs.k1[n,hh],IRFs.k1.hdi[,n,hh])
  plot(hh,IRFs.k1[n,hh], type="l", ylim=ylims, axes=FALSE, xlab="3 years",
       main=rownames(IRFs.k1)[n])
  abline(h = 0, col = "firebrick")
  if (n==N-1 | n==N){
   # axis(1,c(1,2,5,9),c("","1 quarter","1 year","2 years"))
  } else {
    #axis(1,c(1,2,5,9),c("","","",""))
  }
  axis(2,c(ylims[1],0,ylims[2]),round(c(ylims[1],0,ylims[2]),3))
  polygon(c(hh,(h+1):1), c(IRFs.k1.hdi[1,n,hh],IRFs.k1.hdi[2,n,(h+1):1]), col=mcxs1.shade1,border=mcxs1.shade1)
  lines(hh, IRFs.k1[n,hh],lwd=2,col=mcxs1)
}
#dev.off()

# Plots of FEVD of unemployment and rgdp
############################################################
load("irf-fevd-k1.RData")
hh            = 1:(h+1)
fevd.unemp  = apply(FEVD.posterior[1,,,],1:2,mean)
fevd.unemp  = rbind(rep(0,h+1),apply(fevd.unemp,2,cumsum))

fevd.rgdp     = apply(FEVD.posterior[2,,,],1:2,mean)
fevd.rgdp     = rbind(rep(0,h+1),apply(fevd.rgdp,2,cumsum))

colors <- brewer.pal(12, name = "Paired")

# colors = c("deepskyblue1","deepskyblue2","deepskyblue","deepskyblue3","deepskyblue4","dodgerblue",
#            "maroon1","maroon","maroon2","magenta","maroon3","maroon4")

pdf(file="fevd-unemp.pdf", height=7, width=12)
par(mar=rep(4,4),cex.axis=1, cex.lab=0.8)
plot(hh,fevd.unemp[1,], type="n", ylim=c(0,100), axes=FALSE, xlab="", ylab="")
#axis(1,hh,c("","1 quarter","","","1 year","","","","2 years"))
axis(2,c(0,50,100),c("","FEVD[unemp]",""))
for (n in 1:N){
  polygon(c(hh,(h+1):1), c(fevd.unemp[n,hh],fevd.unemp[n+1,(h+1):1]), col=colors[n],border=colors[n])
}
axis(4, (0.5*(fevd.unemp[1:9,9]+fevd.unemp[2:10,9]))[c(3,8,10)], c("","",""))
dev.off()


pdf(file="fevd-rgdp.pdf", height=7, width=12)
par(mar=rep(4,4),cex.axis=1, cex.lab=0.8)
plot(hh,fevd.rgdp[1,], type="n", ylim=c(0,100), axes=FALSE, xlab="", ylab="")
#axis(1,hh,c("","1 quarter","","","1 year","","","","2 years"))
axis(2,c(0,50,100),c("","FEVD[rgdp]",""))
for (n in 1:N){
  polygon(c(hh,(h+1):1), c(fevd.rgdp[n,hh],fevd.rgdp[n+1,(h+1):1]), col=colors[n],border=colors[n])
}
axis(4, (0.5*(fevd.rgdp[1:9,9]+fevd.rgdp[2:10,9]))[c(3,10)], c("",""))
dev.off()


```

```{r Extended model plots}
# mhx.4$Theta[5001:10000,]
# mhx.4$Theta[9001:10000,] #strictly contemporenous, dontrstr
extended.model <- sign.extension(lnY.df_num, p = 4, S = 10000, sign.restrictions=c(0, 0, 1, 1, 1, 1, 1, -1, 1), k1 = 1, 
                                 Theta.mh = mhx.4$Theta)

N <- dim(extended.model$A.posterior)[2]
S <- dim(extended.model$A.posterior)[3]
p <- 4
h <- 12


A.posterior.extended <- extended.model$A.posterior
B0.posterior.extended <- extended.model$B0

B.posterior.extended <- array(NA, c(N,N,S))
for (s in 1:S){
  B.posterior.extended[,,s] <- solve(B0.posterior.extended[,,s])
}

# Impulse response functions
# Forecast Error Variance Decomposition
############################################################
A.posterior <- A.posterior.extended 
B.posterior <- B.posterior.extended

IRF.posterior     = array(NA,c(N,N,h+1,S))
IRF.inf.posterior = array(NA,c(N,N,S))
FEVD.posterior    = array(NA,c(N,N,h+1,S))
J                 = cbind(diag(N),matrix(0,N,N*(p-1)))

#pb = txtProgressBar(min = 0, max = S, initial = 0) 
for (s in 1:S){
  #setTxtProgressBar(pb,s)
  A.bold          = rbind(t(A.posterior[2:(1+N*p),,s]),cbind(diag(N*(p-1)),matrix(0,N*(p-1),N)))
  IRF.inf.posterior[,,s]          = J %*% solve(diag(N*p)-A.bold) %*% t(J) %*% B.posterior[,,s]
  A.bold.power    = A.bold
  for (i in 1:(h+1)){
    if (i==1){
      IRF.posterior[,,i,s]        = B.posterior[,,s]
    } else {
      IRF.posterior[,,i,s]        = J %*% A.bold.power %*% t(J) %*% B.posterior[,,s]
      A.bold.power                = A.bold.power %*% A.bold
    }
    for (n in 1:N){
      for (nn in 1:N){
        FEVD.posterior[n,nn,i,s]  = sum(IRF.posterior[n,nn,1:i,s]^2)
      }
    }
    FEVD.posterior[,,i,s]         = diag(1/apply(FEVD.posterior[,,i,s],1,sum))%*%FEVD.posterior[,,i,s]
  }
}
FEVD.posterior    = 100*FEVD.posterior

#t1          = proc.time()
#(t1-t0)[3]/60 # Time of computations in minutes

# save(IRF.posterior,IRF.inf.posterior, FEVD.posterior, file="irf-fevd-k002.RData")
save(IRF.posterior,IRF.inf.posterior, FEVD.posterior, file="irf-fevd-k1-ext.RData")


load("irf-fevd-k1-ext.RData")

# IRF plots GFCF
############################################################
shock.var <- 5
IRFs.k1           = apply(IRF.posterior[1:N,shock.var,,],1:2,mean)
IRFs.inf.k1       = apply(IRF.inf.posterior[1:N,shock.var,],1,mean)
rownames(IRFs.k1) = varname_vec

IRFs.k1.hdi    = apply(IRF.posterior[1:N,shock.var,,],1:2,hdi, credMass=0.68)
hh          = 1:(h+1)

#pdf(file="irf-GFCF.pdf", height=9, width=12)
par(mfrow=c(3,3), mar=c(2,2,2,2),cex.axis=1.5, cex.lab=1.5)
for (n in 1:N){
  ylims     = range(IRFs.k1[n,hh],IRFs.k1.hdi[,n,hh])
  plot(hh,IRFs.k1[n,hh], type="l", ylim=ylims, axes=FALSE, xlab="3 years",
       main=rownames(IRFs.k1)[n])
  abline(h = 0, col = "firebrick")
  if (n==N-1 | n==N){
   # axis(1,c(1,2,5,9),c("","1 quarter","1 year","2 years"))
  } else {
    #axis(1,c(1,2,5,9),c("","","",""))
  }
  axis(2,c(ylims[1],0,ylims[2]),round(c(ylims[1],0,ylims[2]),3))
  polygon(c(hh,(h+1):1), c(IRFs.k1.hdi[1,n,hh],IRFs.k1.hdi[2,n,(h+1):1]), col=mcxs1.shade1,border=mcxs1.shade1)
  lines(hh, IRFs.k1[n,hh],lwd=2,col=mcxs1)
}
#dev.off()


# Plots of FEVD of Australian rgdp and p
############################################################
hh            = 1:(h+1)
fevd.unemp  = apply(FEVD.posterior[1,,,],1:2,mean)
fevd.unemp  = rbind(rep(0,h+1),apply(fevd.unemp,2,cumsum))

fevd.rgdp     = apply(FEVD.posterior[2,,,],1:2,mean)
fevd.rgdp     = rbind(rep(0,h+1),apply(fevd.rgdp,2,cumsum))

colors <- brewer.pal(12, name = "Paired")

# colors = c("deepskyblue1","deepskyblue2","deepskyblue","deepskyblue3","deepskyblue4","dodgerblue",
#            "maroon1","maroon","maroon2","magenta","maroon3","maroon4")

pdf(file="fevd-ext-unemp.pdf", height=7, width=12)
par(mar=rep(4,4),cex.axis=1, cex.lab=0.8)
plot(hh,fevd.unemp[1,], type="n", ylim=c(0,100), axes=FALSE, xlab="", ylab="")
#axis(1,hh,c("","1 quarter","","","1 year","","","","2 years"))
#axis(2,c(0,50,100),c("","FEVD[unemp]",""))
for (n in 1:N){
  polygon(c(hh,(h+1):1), c(fevd.unemp[n,hh],fevd.unemp[n+1,(h+1):1]), col=colors[n],border=colors[n])
}
axis(4, (0.5*(fevd.unemp[1:9,9]+fevd.unemp[2:10,9]))[c(3,8,10)], c("","",""))
dev.off()


pdf(file="fevd-ext-rgdp.pdf", height=7, width=12)
par(mar=rep(4,4),cex.axis=1, cex.lab=0.8)
plot(hh,fevd.rgdp[1,], type="n", ylim=c(0,100), axes=FALSE, xlab="", ylab="")
#axis(1,hh,c("","1 quarter","","","1 year","","","","2 years"))
axis(2,c(0,50,100),c("","FEVD[rgdp]",""))
for (n in 1:N){
  polygon(c(hh,(h+1):1), c(fevd.rgdp[n,hh],fevd.rgdp[n+1,(h+1):1]), col=colors[n],border=colors[n])
}
axis(4, (0.5*(fevd.rgdp[1:9,9]+fevd.rgdp[2:10,9]))[c(3,10)], c("",""))
dev.off()



```
## References

Abubakar, Attahir B. (2016): Dynamic effects of fiscal policy on output and unemployment in Nigeria: An econometric investigation, CBN Journal of Applied Statistics, ISSN 2476-8472, The Central Bank of Nigeria, Abuja, Vol. 07, Iss. 2, pp. 101-122
