---
title: "Effects of Fiscal Policy on Unemployment and Output in Australia: a Bayesian SVAR Approach"
author: "Ray Gomez"

execute:
  echo: false
  
bibliography: references.bib
---

> **Abstract.** This proposed study attempts to estimate the effects of fiscal policy instruments on unemployment and output in Australia, using a Bayesian Structural Vector Autoregression (BSVAR) model which accounts for heightened volatility due to the COVID-19 pandemic. Impulse response functions are used for measuring these effects.
>
> **Keywords.** Fiscal policy, unemployment, GDP, Australia impulse response, COVID-19, bsvars, R

## Introduction

In many countries, fiscal policy is viewed as a direct means of achieving inclusive development objectives, frequently articulated as twin goals of sustained growth and low poverty (often achieved through low unemployment). However, fiscal policy shocks often occur in multifaceted ways, with simultaneous changes in both revenue and expenditure-side elements of the fiscal balance, making it difficult to disentangle the effects of any one particular policy shock.

This study attempts to identify the effect of the following fiscal policy instrument: tax policy, public investment expenditures, and social transfers, on both unemployment and output in the Australian context, using a Bayesian Structural Vector Autoregression (BSVAR) approach.

The analysis is guided by the work of Abubakar, Attahir B. (2016), which used an SVAR approach to estimate the effect of public expenditures and public revenues on Nigerian output and unemployment and @lenza2022estimate which proposes an estimation procedure for VARs after the COVID-19 pandemic. This study extends their work by using a more extensive dataset, containing more disaggregated revenue and expenditure variables, the inclusion of monetary policy control variables, and applying the COVID volatility model into a structural analysis.

## Data sources

Quarterly data from Q1 1990 to Q4 2022 was sourced from the Australian Bureau of Statistics (ABS) and the Reserve Bank of Australia (RBA) and extracted using the `readabs` and `readrba` packages in `R`.

Unemployment rate and nominal GDP data are viewed as the outcome variables while tax revenue, public gross fixed capital formation (i.e., public investment) and government social assistance payments (i.e., social subsidies) are the explanatory variables of interest. Government final consumption is included to control for the effects of less productive, more routine government spending. Lastly, non-tax revenue (which, arguably, have a less distortionary effect on markets compared to taxes) and monetary policy reflected through the cash rate target and M3 money supply are also controlled for to mitigate omitted variable issues.

Outcome variables:

-   Unemployment rate and nominal GDP

Explanatory variables of interest

-   Revenue: Tax revenue

-   Spending: Public gross fixed capital formation and social assistance payments

Control variables:

-   Other fiscal policies: non-tax (gross income less tax) revenue, government final consumption
-   Monetary policy: cash rate target and M3 money supply
-   External sector: real exchange rate

```{r set up and libraries, include = F}
rm(list=ls())
library(readrba)
library(readabs)
library(xts)
library(tseries)
library(urca)
library(FinTS)
library(rmarkdown)
library(corrplot)
library(parallel)
library(MASS)
library(coda)
library(sads)
library(truncnorm)
library(mvtnorm)
library(plot3D)
library(HDInterval)
library(RColorBrewer)
```

```{r data download, include = F}
#| message: false
#rba <- browse_rba_series(search_string = "")
#abs <- as.matrix(show_available_catalogues())
#remotes::install_github("mattcowgill/readrba")

## Monetary policy variables
# Cash interest rate
cash_rate.dl   <- read_rba(series_id = "FIRMMCRTD")
cashrate <- to.quarterly(xts(cash_rate.dl$value, cash_rate.dl$date), 
                         OHLC = FALSE)
# M3 Money supply
M3.dl   <- read_rba(series_id = "DMAM3S")
M3 <- to.quarterly(xts(M3.dl$value, M3.dl$date), 
                         OHLC = FALSE)

# real exchange rate (TWI)
# realTWI.dl   <- read_rba(series_id = "FRERTWI")
# realTWI <- to.quarterly(xts(realTWI.dl$value, realTWI.dl$date), 
                         # OHLC = FALSE)

# Unemployment rate: A84423092X 
# sadj A84423050A
unemp_rate.dl <- read_abs(series_id = "A84423050A")
unemp <- to.quarterly(xts(unemp_rate.dl$value, unemp_rate.dl$date), 
                           OHLC = FALSE)

# real GDP: A2302459A
# sadj A2304402X
realGDP.dl   <- read_abs(series_id = "A2304402X")
realGDP <- to.quarterly(xts(realGDP.dl$value, realGDP.dl$date), 
                         OHLC = FALSE)

# Nominal GDP: A2302467A
# nomGDP.dl <- read_abs(series_id = "A2302467A")
# nomGDP <- to.quarterly(xts(nomGDP.dl$value, nomGDP.dl$date), 
#                          OHLC = FALSE)

#summary(unemp_rate)
## Fiscal variables
# Total tax: A2301963V 
totaltax.dl <- read_abs(series_id = "A2302794K")
totaltax <- to.quarterly(xts(totaltax.dl$value, totaltax.dl$date), 
                         OHLC = FALSE)

# Non-tax revenue: Gross income (A2302106V) - total tax 
govgrossinc.dl <- read_abs(series_id = "A2302742J")
govgrossinc <- to.quarterly(xts(govgrossinc.dl$value, govgrossinc.dl$date), 
                         OHLC = FALSE)

nontax <- govgrossinc - totaltax

# Public gross fixed capital formation: A2302555A
pubinv.dl <- read_abs(series_id = "A2304065W")
pubinv <- to.quarterly(xts(pubinv.dl$value, pubinv.dl$date), 
                         OHLC = FALSE)

# Social assistance benefits payments: A2301976F
pubtrans.dl <- read_abs(series_id = "A2301976F")
pubtrans <- to.quarterly(xts(pubtrans.dl$value, pubtrans.dl$date), 
                         OHLC = FALSE)

# Government final consumption: A2302527T
pubcons.dl <- read_abs(series_id = "A2304036K")
pubcons <- to.quarterly(xts(pubcons.dl$value, pubcons.dl$date),
                         OHLC = FALSE)

```

##### Table 1. Data from ABS and RBA {style="text-align: center;"}

```{r data merge and log transform, echo = F}
# Merge data into one matrix
Y.df  <- na.omit(merge(unemp, realGDP , totaltax, nontax, pubinv, 
                       pubtrans, pubcons, cashrate, M3))

varname_vec <- c("Unemployment rate", "Real GDP", "Tax revenue", "Non-tax revenue", "Gov't GFCF",
                    "Social benefits payments", "Gov't consumption", "Cash rate target", "M3 supply")
colnames(Y.df) <- varname_vec

Y.df <- Y.df[1:132,]
# Transform into natural logs
lnY.df <- log(Y.df)

date <- as.vector(index(cashrate))[1:132]
T <- length(date)

paged_table(as.data.frame(round(head(lnY.df,4),2)))
paged_table(as.data.frame(round(tail(lnY.df,4),2)))

# data for input to estimation function 
lnY.df_num <- c()

for (i in 1:ncol(lnY.df)){
  lnY.df_num <- cbind(lnY.df_num, as.numeric(lnY.df[,i]))
}

# Return unemployment rate and cash rate target to non-log terms
lnY.df_num[,1] <- Y.df[,1] 
lnY.df_num[,8] <- Y.df[,8] 

```

# 

As seen in the figures below, Australian GDP output has been on a relatively steady, upward trend since 1990, along with fiscal revenue and spending. These have, by and large, been accommodated by declining interest rates and an expanding money supply. Overall, these have been associated with a downward, albeit volatile trajectory in the unemployment rate. Currently, the data is left in original, seasonally-unadjusted form as the variable for social benefit payments does not have a seasonally-adjusted version in the ABS website. As such, some seasonal variation between quarters can be observed in time series plots. However, the final version of the data to be used will be determined after further exploration. Lastly, the data will be transformed into natural log terms during the analysis as to interpret the coefficients in the structural matrix as elasticities.

##### Figure 1. Time series plots: 1990 Q1 to 2022 Q4, original values {style="text-align: center;"}

```{r, echo = F}
colvec <- c("turquoise4","turquoise4", "plum4", "plum4", "plum4", "plum4", "plum4","firebrick3","firebrick3")
par(mfrow=c(3,3), mar=c(2,2,2,2))
for (i in 1:9){
  plot(date, y = Y.df[,i], type = "l", 
       main = paste(varname_vec[i]), ylab = "", xlab = "",
       col = colvec[i], lwd = 1.5,
       ylim = c(min(Y.df[,i]),max(Y.df[,i])))
}

```

## Preliminary data analysis

### ACF and PACF analysis

Across variables (which have been transformed into natural log terms), ACF plots show a strong, positive, and gradually decaying autocorrelation structure. Moreover various PACF plots indicate a near-one value at the first lag and much lower, mostly statistically zero values at higher lag orders (such as in nominal GDP, social benefits payments, government consumption, and M3). These characteristics may indicate that some variables may be following a random walk with drift process and as such may be unit-root non-stationary.

###### Figure 2.1. Autocorrelation function (ACF) plots of variables in log terms {style="text-align: center;"}

```{r ACF plots, echo = FALSE}
par(mfrow=c(3,3), mar=c(2,2,2,2))
for (i in 1:9){
  Acf(lnY.df[,i], main = "", ylim = c(-0.2,1))
  title(main = paste(varname_vec[i]), line = 0.5)
} 
```

###### Figure 2.2. Partial autocorrelation function (PACF) plots of variables in log terms {style="text-align: center;"}

```{r PACF plotsPACF analysis, echo = FALSE}
par(mfrow=c(3,3), mar=c(2,2,2,2))
for (i in 1:9){
  pacf(lnY.df[,i], main = "", ylim = c(-0.2,1))
  title(main = paste(varname_vec[i]), line = 0.5)
} 

```

### Augmented Dickey-Fuller test for unit roots

```{r Ng-Perron ADF test function, echo = TRUE}
# function to implement the Ng and Perron (1998) ADF test procedure
ngperronADF <- function(y){
  T <- length(y)
  kmax <- ceiling(12*(((T-25)/100)^(1/4)))
  k <- kmax + 1
  # run test regression until t-statistic of kth lag > 1.6
  t <- 0
  while(abs(t) < 1.6){
      k <- k - 1
      adft <- ur.df(y, type = "drift", lags = k)
      t <- as.numeric(adft@testreg$coefficients[k+2,][3])
  }
  adft_k <-ur.df(y, type = "drift", lags = k)
  teststat <- adft_k@teststat[2]
  cval <- adft_k@cval[2,2]
  sig <- abs(teststat)>abs(cval)
  return(list("stat" = teststat,"crit" = cval,"reject" = sig))
}

```

##### Table 2. ADF test results: levels, first, and second difference {style="text-align: center;"}

```{r ADF test and order of integration determination, echo = FALSE}

adf_results_lev <- c()
adf_results_fd <- c()
adf_results_fd2 <- c()

for (i in 1:9){
npadf_lev <- ngperronADF(lnY.df_num[,i])  
npadf_fd <- ngperronADF(diff(lnY.df_num[,i])[-1]) 
npadf_fd2 <- ngperronADF(diff(diff(lnY.df_num[,i])[-1])[-1]) 


adf_results_lev <- round(rbind(adf_results_lev,c(npadf_lev[[1]],
                                           npadf_lev[[3]])),2)

adf_results_fd <- round(rbind(adf_results_fd,c(npadf_fd[[1]],
                                         npadf_fd[[3]])),2)
adf_results_fd2 <- round(rbind(adf_results_fd2,c(npadf_fd2[[1]],
                                        npadf_fd2[[3]])),2)
}

colnames(adf_results_lev) <- c("ADF statistic, levels", "Reject?")
colnames(adf_results_fd) <- c("ADF statistic, 1st difference", "Reject?")
colnames(adf_results_fd2) <- c("ADF statistic, 2nd difference", "Reject?")

rownames(adf_results_lev) <- varname_vec
rownames(adf_results_fd) <- varname_vec
rownames(adf_results_fd2) <- varname_vec

adf_results <- cbind(adf_results_lev,adf_results_fd, adf_results_fd2)

paged_table(as.data.frame(adf_results))


for (i in 1:9){
npadf_lev <- ngperronADF(lnY.df_num[,i])  
#npadf_fd <- ngperronADF(diff(lnY.df[,i])[-1]p)  
}
```

# 

Augmented Dickey-Fuller tests following the maximum lag-setting procedure of Ng and Perron (1998) were performed on all nine variables to determine stationarity and orders of integration. The null hypothesis that a unit roots exists was rejected for ADF tests on levels of tax revenue, non-tax revenue, social benefits payments, government consumption, and M3 supply, indicating that these variables do not have a unit-root, and may be following trend-stationary autoregressive processes. Meanwhile, the first differences of the unemployment rate, government GFCF, and cash rate target variables yielded a significant ADF test result, indicating that these have order of integration of 1. Lastly, it took another differencing for nominal GDP to yield a significant result, indicating an order of integration of 2. Further analysis of these variables, including a better-calibrated ADF test and cointegration tests will be performed in the future.

## Model specification

The use of fiscal policy to stimulate or regulate the economy follows the Keynesian approach generally practiced around the world (but especially in developing countries) to navigate economies through downturns or potential episodes of overheating, particularly in the short-run. The proposed model aims to investigate the effectiveness of these fiscal measures in managing growth and unemployment.

Estimation proceeds through the following reduced form model:

$$
\begin{align}
Y &= XA + U\\
U| X &\sim _{iid} MN_{T\times N}( 0, \Sigma, \Omega)\\
\Sigma &= B_0^{-1}B_0^{-1'}
\end{align}
$$ where matrix $Y$ contains vectors of endogenous variables $y_t$:

$$y_t=\begin{pmatrix} unemp_t &= \text{unemployment rate}
\\ realgdp_t &= \text{real GDP}
\\ totaltax_t  &= \text{tax revenue}
\\ nontax_t  &= \text{non-tax revenue}
\\ pubinv_t  &= \text{government gross fixed capital formation}
\\ pubtrans_t  &= \text{social assitance and benefits payments}
\\ pubcons_t  &= \text{government final consumption}
\\ cashrate_t  &= \text{cash rate target}
\\ M3_t  &= \text{M3 money supply}
\end{pmatrix}$$

and $B_0^{-1}=B$ is the matrix of contemporaneous effects between variables.

## Estimation and identification

### Basic model

For the basic model, the column-wise covariance matrix is set as $\Omega=I_N$. The likelihood function is expressed as:

$$P(Y,X|A,\Sigma) \propto det(\Sigma)^{-\frac{T}{2}} exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(Y-XA)'(Y-XA) \right] \right\}\\$$ $$=det(\Sigma)^{-\frac{T}{2}} exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(A-\hat{A})'X'X(A-\hat{A}) \right] \right\} exp \left\{-\frac{1}{2} tr \left[\Sigma^{-1}(Y-X \hat{A})'(Y-X \hat{A}) \right] \right\}$$

where the maximum likelihood estimators are expressed as:

$$\hat{A} = (X'X)^{-1}X'Y $$ $$\hat{\Sigma} = \frac{1}{T} (Y-X \hat{A})'(Y-X \hat{A})$$

The prior for $A$ and $\Sigma$ is assumed to be a matrix-normal inverse Wishart distribution that follows the Minnesota specification of @doan1984forecasting:

$$p(A,\Sigma) =MNIW(\underline{A},\underline{S},\underline{V},\underline{\nu})$$

$$A|\Sigma\sim MN_{K\times N}(\underline{A},\Sigma,\underline{V})$$ $$\Sigma\sim IW_N(\underline{S},\underline{\nu})$$

where:

$$\underline{A} = [0_{N \times 1} \quad I_N \quad 0_{N \times (p-1)N}]'$$ $$\underline{V} = diag([\kappa_2 \quad \kappa_1 (p^{-2} \otimes I_N)])$$

This leads to the following posterior distribution, from which the basic reduced form model will be estimated:

$$P(A|Y,X,\Sigma)=MN_{K\times N}(\bar{A},\Sigma,\bar{V})$$ $$P(\Sigma|Y,X)=IW_N(\bar{S},\bar{\nu})\\$$

$$\bar{V}=(X'X+\underline{V}^{-1})^{-1}$$ $$\bar{A}=\bar{V}(X'Y+\underline{V}^{-1}\underline{A})$$ $$\bar{\nu}=T+\underline{\nu}$$ $$\bar{S}=\underline{S}+Y'Y+\underline{A}'\underline{V}^{-1}\underline{A}-
\bar{A}'\bar{V}^{-1}\bar{A}$$

### Extended model

For the extended model, the column-wise covariance matrix is set to $\Omega=\text{diag}(H^2)$, where $H$ is a vector of COVID volatility variables:

$$H=[1\quad...\quad1 \quad h_0\quad h_1\quad h_2\quad 1+(h_2-1)\rho\quad 1+(h_2-1)\rho^2\quad...]'$$

Intuitively, volatility variables for periods before COVID are set to unity. Heightened volatility during the first three quarters of COVID are parameterized, before they are assumed to decay gradually beginning at the fourth quarter since the pandemic's onset.

These COVID volatility parameters $\theta=(h_0\quad h_1\quad h_2\quad\rho)$ are estimated from their own marginal posterior as proposed by @lenza2022estimate:

$$P(\theta|Y,X,\underline{\gamma})\propto P(Y,X|\theta,\underline{\gamma})P(\theta|\underline{\gamma})$$

where the likelihood function is given as:

$$P(Y,X|\theta,\underline{\gamma})\propto \Bigg(\prod^T_{t=1}h_t^{-N}\Bigg)||\underline{V}||^{\frac{N}{2}}||\underline{S}||^{\frac{\underline{\nu}}{2}}||\tilde{X}'\tilde{X}+\underline{V}^{-1}||^{\frac{N}{2}}\\$$

$$||\underline{S}+\hat{\tilde{E}}'\hat{\tilde{E}}+(\hat{\tilde{A}}-\tilde{Y}+\tilde{X}\underline{A})'\underline{V}^{-1}(\hat{\tilde{A}}-\tilde{Y}+\tilde{X}\underline{A})||^{-\frac{T-p+\underline{\nu}}{2}}$$

where $\tilde{X}_t=\frac{(1,Y_t,...,Y_{t-p})'}{h_t}$, $\tilde{\hat{A}}=(\tilde{X}'\tilde{X}-\underline{V}^{-1})^{-1}$, and $\underline{\gamma}=(\underline{A},\underline{S},\underline{V},\underline{\nu})$

and the priors are assumed to be: $h_0,h_1,h_2\sim Pareto(1,1)$ and $\rho\sim Beta(3,1.5)$

Sampling from the above posterior is conducted via the following Metropolis MCMC algorithm:

1.  Initialize $\theta$ at the posterior mode which was located via numerical optimization

2.  Draw candidate $\theta^{*}$ from $N_4(\theta^{(s-1)},cW)$, where $W$ is the inverse Hessian of the negative log posterior of $\theta$ at the mode, which is also calculated computationally, and $c$ is a scaling factor.

3.  Set:

$$\theta^{(s)}=  \begin{cases}
        \theta^* & \text{with pr.} \quad \alpha^{(s)} 
        \\
        \\
        \theta^{(s-1)} & \text{with pr.} \quad 1-\alpha^{(s)} 
  \end{cases}$$

$$\alpha^{(s)} =\text{min}\Big[1,\frac{P(\theta^*|Y,X,\underline{\gamma})}{P(\theta^{(s-1)}|Y,X,\underline{\gamma})}\Big]$$

4.  Define $H^{(s)}$ matrix using $\theta^{(s)}$:

$$H^{(s)}=[1\quad...\quad1 \quad h_0^{(s)}\quad h_1^{(s)}\quad h_2^{(s)}\quad 1+(h_2^{(s)}-1)\rho^{(s)}\quad 1+(h_2^{(s)}-1)\rho^{(s)2}\quad...]'$$

Steps 2 to 4 are repeated $S_1+S_2$ times and $S_2$ draws are used to draw $A$ and $\Sigma$ from the following posterior distribution:

$$P(A|Y,X,\Sigma,H)=MN_{K\times N}(\bar{A},\Sigma,\bar{V})$$ $$P(\Sigma|Y,X,H)=IW_N(\bar{S},\bar{\nu})\\$$

$$\bar{V}=(X'\text{diag}(H^2)X+\underline{V}^{-1})^{-1}$$ $$\bar{A}=\bar{V}(X'\text{diag}(H^2)Y+\underline{V}^{-1}\underline{A})$$ $$\bar{\nu}=T+\underline{\nu}$$ $$\bar{S}=\underline{S}+Y'\text{diag}(H^2)Y+\underline{A}'\underline{V}^{-1}\underline{A}-
\bar{A}'\bar{V}^{-1}\bar{A}$$

### Identification

Sign restrictions are used as the identification strategy for both models.

Given the reduced form model:

$$
\begin{align}
Y &= XA + U\\
U| X &\sim _{iid} MN_{T\times N}( 0, \Sigma, \Omega)\\
\end{align}
$$ For each draw from the reduced form posteriors, the candidate contemporaneous effects matrix is derived as: $$\tilde{B}=\tilde{B}^{-1}_0=\text{chol}(\Sigma)$$

Contemporaneous effects matrix $B$ is identified by searching for an appropriate rotation matrix $Q$ such that prescribed sign restrictions hold:

$$B = Q\tilde{B}$$

such that:

$$R_nf(\tilde{B}_0,\tilde{B}_+)e_n\geq0$$ $$f(\tilde{B}_0,\tilde{B}_+)=\Theta_0=\tilde{B}^{-1}_0$$

### Estimation with artificial data

To check the validity of the algorithms, two independent bi-variate random walk with drift processes were generated to simulate unit-root non-stationary macroeconomic variables.

```{r RW sample}
set.seed(1)
y1 <- arima.sim(model= list(order = c(0, 1, 0)), n=1000, mean=1,sd=1)
y2 <- arima.sim(model= list(order = c(0, 1, 0)), n=1000, mean=1,sd=1)
y <- cbind(y1,y2)

par(mfrow=c(1,2))
plot(y=y1, x = 1:1001, type = "l", col = "turquoise4", 
     lwd = 1.5, ylab = "", xlab = "", main = "RW with drift 1")
plot(y=y2, x = 1:1001, type = "l", col = "plum4", 
     lwd = 1.5, ylab = "",xlab = "", main = "RW with drift 2")
```

The estimation procedure for the basic model is implemented in `R` by the following function, `sign.basic`:

```{r Basic model estimation function, echo = TRUE}
#| code-fold: true
#| code-summary: "Show code"

# data = input data should be quarterly
# p = lags
# S = number of posterior draws
# sign restritions = Nx1 diagonal of R matrix
# k1 = kappa1
# k2 = kappa2, higher value, less shrinkage, more weight on prior
# shockvar = variable being shocked
# start date = start date of Y matrix
sign.basic <- function(data, p, S,  sign.restrictions,
                           k1=0.04, k2=100, start_date = c(1991,1), shockvar=5){
  # Define Y and X matrices
  ############################################################
  # N = no. of variables
  N = ncol(data)
  # p = no. of lags
  K = 1 + p*N
  # forecast horizon
  # h       = 8
  
  Y       = ts(data[(p+1):nrow(data),], start=start_date, frequency=4)
  X       = matrix(1,nrow(Y),1)
  # nrow(X)
  for (i in 1:p){
    X     = cbind(X,data[(p+1):nrow(data)-i,])
  }
  
  # Calculate MLE
  ############################################################
  A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
  Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)
  # round(A.hat,3)
  # round(Sigma.hat,3)
  # round(cov2cor(Sigma.hat),3)
  
  # Specify prior distribution
  ############################################################
  kappa.1     = k1
  kappa.2     = k2
  kappa.3     = 1
  A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))
  A.prior[2:(N+1),] = kappa.3*diag(N)
  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
  S.prior     = diag(diag(Sigma.hat))
  nu.prior    = N+1
  
  # Matrix normal-inverse Wishart posterior parameters
  ############################################################
  V.bar.inv   = t(X)%*%X + diag(1/diag(V.prior))
  V.bar       = solve(V.bar.inv)
  A.bar       = V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)
  nu.bar      = nrow(Y) + nu.prior
  S.bar       = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
  S.bar.inv   = solve(S.bar)
  
  # Posterior draws 
  ############################################################
  # Draws from RF posterior
  # Draw Sigma from inverse wishart
  Sigma.posterior   = rWishart(S, df=nu.bar, Sigma=S.bar.inv)
  Sigma.posterior   = apply(Sigma.posterior,3,solve)
  Sigma.posterior   = array(Sigma.posterior,c(N,N,S))
  
  # Draw A from matrix-variate normal
  A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S))
  
  ## Draw from SF posterior
  B0.posterior       = array(NA,c(N,N,S))
  Bplus.posterior       = array(NA,c(N,K,S))
  L                 = t(chol(V.bar))
  for (s in 1:S){
    # Draw B0
    cholSigma.s     = chol(Sigma.posterior[,,s])
    B0.posterior[,,s]= solve(t(cholSigma.s))
    A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%cholSigma.s
    # Draw Bplus
    Bplus.posterior[,,s] = B0.posterior[,,s]%*%t(A.posterior[,,s])
  }

  # Identification via sign restrictions 
  ############################################################
  
  # Generate corresponding R matrix
  R1 = diag(sign.restrictions)
  
  # Storage matrices for Q identified estimates
  i.vec <- c()
  Q.iden   = array(NA,c(N,N,S))
  B0.iden = array(NA,c(N,N,S))
  B1.iden = array(NA,c(N,K,S))
  A.iden = array (NA,c(K,N,S))
  
  pb = txtProgressBar(min = 0, max = 100, initial = 0) 

  for (s in 1:S){
    
    setTxtProgressBar(pb,100*s/S)
    #cat(". iteration: ", s, "\n")
    
    # pick-up a B0 from S
    B0.tilde <- B0.posterior[,,s]
    IR.0.tilde    = solve(B0.tilde)
    B1.tilde      = Bplus.posterior[,,s]
    #IR.1.tilde    = solve(B0.tilde)%*%B1.tilde%*%solve(B0.tilde)

    # Search for appropriate Q 
    sign.restrictions.do.not.hold = TRUE
    i=1
    while (sign.restrictions.do.not.hold){
      X           = matrix(rnorm(N^2),N,N)
      QR          = qr(X, tol = 1e-10)
      Q           = qr.Q(QR,complete=TRUE)
      R           = qr.R(QR,complete=TRUE)
      Q           = t(Q %*% diag(sign(diag(R))))
      B0          = Q%*%B0.tilde
      B1          = Q%*%B1.tilde
      B0.inv      = solve(B0)
      check       = prod(R1 %*% B0.inv %*% diag(N)[,shockvar] >= 0)
      A           = t(solve(B0)%*%B1)

      if (check==1){sign.restrictions.do.not.hold=FALSE}
      i=i+1
    }
    i.vec <- c(i.vec, i)
    Q.iden[,,s] <- Q
    B0.iden[,,s] <- B0
    B0.mean <- apply(B0.iden,1:2,mean)
    B1.iden[,,s] <- B1
    B1.mean <- apply(B1.iden,1:2,mean)
    A.iden[,,s] <- A
    A.mean <- apply(A.iden,1:2,mean)
    

  }
  re <- list("i" = i.vec, "Q" = Q.iden, "B0"= B0.iden, "B0.mean" = B0.mean,
             "Bplus"= B1.iden, "Bplus.mean" = B1.mean, "A" = A.iden, "A.mean" = A,
             "A.posterior"=A.posterior, "Sigma.posterior"=Sigma.posterior)
  return(re)
}

```

Applying the `sign.basic` function on the artificial returns the following matrices as estimates for the posterior means of $A$ and $\Sigma$. These results are appropriate given the independent bivariate random walk with drift proceses:

```{r}
set.seed(1)
RW_estimation <-  sign.basic(data=y, p=1, S=10000, c(0,1), k1 = 1, k2 = 100, shockvar = 1)

A.mean_tab <- as.data.frame(round(RW_estimation$A.mean,3))
colnames(A.mean_tab) <- c("RW1", "RW2")

Sigma.mean_tab <- as.data.frame(round(apply(RW_estimation$Sigma.posterior,1:2,mean),3))
colnames(Sigma.mean_tab) <- c("RW1", "RW2")

paged_table(A.mean_tab)
paged_table(Sigma.mean_tab)
```

Estimating the extended model involved designing three functions in `R`:

1.  `v.posterior.mode` takes in the data and numerically computes for the posterior mode of $\theta$, i.e., COVID volatility parameters;

```{r COVID parameters posterior maximization, echo = TRUE}
#| code-fold: true
#| code-summary: "Show code"
v.posterior.mode <- function(data, p=4, k1=1, k2=100, start_date=c(1991,1)){

  v.neglogPost <- function(theta){
    N = ncol(data)
    # p = no. of lags
    K = 1 + p*N
    # forecast horizon
    # h       = 8
    Y       = ts(data[(p+1):nrow(data),], start=start_date, frequency=4)
    T = nrow(Y)
    X       = matrix(1,T,1)
    # nrow(X)
    for (i in 1:p){
      X     = cbind(X,data[(p+1):nrow(data)-i,])
    }
    
    # Calculate MLE for prior 
    ############################################################
    A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
    Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)
    # round(A.hat,3)
    # round(Sigma.hat,3)
    # round(cov2cor(Sigma.hat),3)
    
    # Specify prior distribution
    ############################################################
    kappa.1     = k1
    kappa.2     = k2
    kappa.3     = 1
    A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))
    A.prior[2:(N+1),] = kappa.3*diag(N)
    V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
    S.prior     = diag(diag(Sigma.hat))
    nu.prior    = N+1
    
    vec <- theta[1:3]
    for (i in 4:12){
      vec <- c(vec, 1 + (theta[3]-1)*theta[4]^(i-3))
    }  
    
    V <- c(ts(rep(1, nrow(Y)-12), c(1991,1), frequency = 4) , vec)    
    
    Y.tilde <- diag(1/V)%*%Y
    X.tilde <- diag(1/V)%*%X
    A.tilde.hat <- solve((t(X.tilde)%*%X.tilde+solve(V.prior)))%*%(t(X.tilde)%*%Y.tilde+solve(V.prior)%*%A.prior)
    epsilon.tilde <-Y.tilde - X.tilde%*%A.tilde.hat
    
    # Log-likelihood      
    logL <- log(prod(V^(-N)))+(-N/2)*log(det(t(X.tilde)%*%X.tilde+solve(V.prior)))+
            (-(T-p+nu.prior)/2)*log(det(S.prior +t(epsilon.tilde)%*%epsilon.tilde + 
            t(A.tilde.hat-A.prior)%*%solve(V.prior)%*%(A.tilde.hat-A.prior)))
    
    # Pareto(1,1) and Beta(3,1.5) priors 
    pareto.a=1
    pareto.b=1
    beta.a=3
    beta.b=1.5
    beta.cons <- 1/beta(beta.a,beta.b)
    
    # Log-prior
    logP <- log((pareto.a*pareto.b^pareto.a)/(theta[1]^(pareto.a+1))*
    (pareto.a*pareto.b^pareto.a)/(theta[2]^(pareto.a+1))*
    (pareto.a*pareto.b^pareto.a)/(theta[3]^(pareto.a+1))*
    beta.cons*theta[4]^(beta.a-1)*(1-theta[4])^(beta.b-1))
    
    # negative log-posterior
    neglogPost <- -(logL+logP)
    
    return(neglogPost)
  }
   
  # numerically minimize the negative log-likelihood
  post.maximizer <- optim(par=c(50, 50, 50, 0.5), fn=v.neglogPost, method="L-BFGS-B", 
                          lower=c(1, 1, 1, 0.0001),
                          upper=c(100,100,100,0.99999), hessian = TRUE)
  
  return(list(maximizer=post.maximizer$par, hessian=post.maximizer$hessian))

}
```

2.  `mh.mcmc` takes in data, the posterior mode of $\theta$, and the inverse Hessian from `v.posterior.mode` to run the above-mentioned Metropolis MCMC algorithm for a specified number of iterations; and

```{r Metropolis-Hastings function, echo = TRUE}
#| code-fold: true
#| code-summary: "Show code"
mh.mcmc <- function(data, p=1, S.mh = 1000, c, W = diag(4), theta.init,
                    k1 = 1, k2 = 100, start_date = c(1991,1)){
 # N = no. of variables
  N = ncol(data)
  # p = no. of lags
  K = 1 + p*N
  # forecast horizon
  # h       = 8
  Y       = ts(data[(p+1):nrow(data),], start=start_date, frequency=4)
  T = nrow(Y)
  X       = matrix(1,T,1)
  # nrow(X)
  for (i in 1:p){
    X     = cbind(X,data[(p+1):nrow(data)-i,])
  }
  

  # Calculate MLE for prior 
  ############################################################
  A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
  Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)
  # round(A.hat,3)
  # round(Sigma.hat,3)
  # round(cov2cor(Sigma.hat),3)
  
  # Specify prior distribution
  ############################################################
  kappa.1     = k1
  kappa.2     = k2
  kappa.3     = 1
  A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))
  A.prior[2:(N+1),] = kappa.3*diag(N)
  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
  S.prior     = diag(diag(Sigma.hat))
  nu.prior    = N+1
  
  # Metropolis-Hastings 
  ###########################################################
  # v0, v1, v2, rho
  Theta <- matrix(NA,S.mh,4)
  theta_old <- theta.init
  #theta_old <- Theta[nrow(Theta),]
  
  # W <- diag(4)
  set.seed(1)
  pb = txtProgressBar(min = 0, max = S.mh, initial = 0) 
  for (s in 1:S.mh){
    setTxtProgressBar(pb,s)

    covid.vec <- function(theta){
      vec <- theta[1:3]
      for (i in 4:12){
        vec <- c(vec, 1 + (theta[3]-1)*theta[4]^(i-3))
      }
      
      return(vec)
    }

    # Covid volatility likelihood kernel
    v.logL <- function(V){
      Y.tilde <- diag(1/V)%*%Y
      X.tilde <- diag(1/V)%*%X
      A.tilde.hat <- solve((t(X.tilde)%*%X.tilde+solve(V.prior)))%*%(t(X.tilde)%*%Y.tilde+solve(V.prior)%*%A.prior)
      epsilon.tilde <-Y.tilde - X.tilde%*%A.tilde.hat

      logL <- log(prod(V^(-N)))+(-N/2)*log(det(t(X.tilde)%*%X.tilde+solve(V.prior)))+
              (-(T-p+nu.prior)/2)*log(det(S.prior +t(epsilon.tilde)%*%epsilon.tilde + 
              t(A.tilde.hat-A.prior)%*%solve(V.prior)%*%(A.tilde.hat-A.prior)))

      return(logL)
    }
  
    # Covid volatility prior
    v.logP <- function(theta, pareto.a=1, pareto.b=1, beta.a=3, beta.b=1.5){
      beta.cons <- 1/beta(beta.a,beta.b)
  
      logP <- log((pareto.a*pareto.b^pareto.a)/(theta[1]^(pareto.a+1))*
      (pareto.a*pareto.b^pareto.a)/(theta[2]^(pareto.a+1))*
      (pareto.a*pareto.b^pareto.a)/(theta[3]^(pareto.a+1))*
       beta.cons*theta[4]^(beta.a-1)*(1-theta[4])^(beta.b-1))
      
      return(logP)
    }

    v_ones <- ts(rep(1, nrow(Y)-12), c(1991,1), frequency = 4) 
    V.old <- c(v_ones, covid.vec(theta_old))    
      
    # New candidate parameters values
    theta_new <- mvrnorm(1, theta_old, c*W)
    V.new <- c(v_ones, covid.vec(theta_new))
    
    # Calculate posteriors 
    v.logpost_old <- v.logL(V.old)+v.logP(theta_old)
    v.logpost_new <- v.logL(V.new)+v.logP(theta_new)
    
    # Posterior ratio
    post.ratio <- exp(v.logpost_new-v.logpost_old)
    
    # Acceptance/rejection alpha
    alpha <- min(1, post.ratio)
    
    u_star <- runif(1)
    
    if (alpha > u_star){
      Theta[s,] <- theta_new
    } else {Theta[s,] <- theta_old}
    
    theta_old <- Theta[s,]  
  }
  
  colnames(Theta) <- c("h0", "h1" , "h2", "rho")

  re <- list(Theta=Theta, 
             AcceptRate = 1 - rejectionRate(as.mcmc(Theta[,1])))
  return(re)
}

```

3.  `sign.extension` takes in data and draws of $\theta$ from `mh.mcmc` to return the posterior draws for the extended model.

```{r Extended model, echo = TRUE}
#| code-fold: true
#| code-summary: "Show code"
sign.extension <- function(data, p=4, S=100,  sign.restrictions = c(0, 0, 1, 1, 1, -1, 1, -1, 1),
                           k1=1, k2=100, shockvar = 5, start_date = c(1991,1), Theta.mh){

  # N = no. of variables
  N = ncol(data)
  # p = no. of lags
  K = 1 + p*N
  # forecast horizon
  # h       = 8
  Y       = ts(data[(p+1):nrow(data),], start=start_date, frequency=4)
  T = nrow(Y)
  X       = matrix(1,T,1)
  # nrow(X)
  for (i in 1:p){
    X     = cbind(X,data[(p+1):nrow(data)-i,])
  }
  
  
  covid.vec <- function(theta){
    vec <- theta[1:3]
    for (i in 4:12){
      vec <- c(vec, 1 + (theta[3]-1)*theta[4]^(i-3))
    }
      
    return(vec)
  }
  

  # array of S diag(covid volatility) matrices
  diagV.sqinv <- array(NA, c(nrow(Y),nrow(Y),S))
  
  for (s in 1:S){
    v_ones <- ts(rep(1, nrow(Y)-12), c(1991,1), frequency = 4) 
    diagV.sqinv[,,s] <- diag(c(v_ones, covid.vec(Theta.mh[s,]))^(-2))
  }
  
  # Calculate MLE for prior 
  ############################################################
  A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
  Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)
  # round(A.hat,3)
  # round(Sigma.hat,3)
  # round(cov2cor(Sigma.hat),3)
  
  # Specify prior distribution
  ############################################################
  kappa.1     = k1
  kappa.2     = k2
  kappa.3     = 1
  A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))
  A.prior[2:(N+1),] = kappa.3*diag(N)
  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
  S.prior     = diag(diag(Sigma.hat))
  nu.prior    = N+1
  
  # Posterior draws 
  ############################################################
  Sigma.posterior   = array(NA,c(N,N,S))
  A.posterior       = array (NA,c(K,N,S))
  B0.posterior       = array(NA,c(N,N,S))
  Bplus.posterior       = array(NA,c(N,K,S))
  
  pb = txtProgressBar(min = 0, max = S, initial = 0) 
  for (s in 1:S){
    setTxtProgressBar(pb,s)
    V.bar.inv   = t(X)%*%diagV.sqinv[,,s]%*%X + diag(1/diag(V.prior))
    V.bar       = solve(V.bar.inv)
    A.bar       = V.bar%*%(t(X)%*%diagV.sqinv[,,s]%*%Y + diag(1/diag(V.prior))%*%A.prior)
    nu.bar      = nrow(Y) + nu.prior
    S.bar       = S.prior + t(Y)%*%diagV.sqinv[,,s]%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%
                  A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
    S.bar.inv   = solve(S.bar)
    L                 = t(chol(V.bar))
    
    # RF posterior draws
    Sigma.posterior[,,s] <- solve(rWishart(1, df=nu.bar, Sigma=S.bar.inv)[,,1])
    cholSigma.s     = chol(Sigma.posterior[,,s])
    A.posterior[,,s]       = matrix(mvrnorm(1,as.vector(A.bar), Sigma.posterior[,,s]%x%V.bar),ncol=N)
    A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%cholSigma.s
    
    # SF posterior draws 
    B0.posterior[,,s]= solve(t(cholSigma.s))
    # Draw Bplus
    Bplus.posterior[,,s] = B0.posterior[,,s]%*%t(A.posterior[,,s])
  }

  # Identification via sign restrictions on theta0
  ############################################################

  # generate corresponding R matrix
  R1 = diag(sign.restrictions)
  
  # storage matrices for Q identified estimates
  i.vec <- c()
  Q.iden   = array(NA,c(N,N,S))
  B0.iden = array(NA,c(N,N,S))
  B1.iden = array(NA,c(N,K,S))
  A.iden = array (NA,c(K,N,S))
  
  pb = txtProgressBar(min = 0, max = S, initial = 0) 

  for (s in 1:S){
    
    setTxtProgressBar(pb,s)
    
    # pick-up a B0 from S
    B0.tilde <- B0.posterior[,,s]
    IR.0.tilde    = solve(B0.tilde)
    B1.tilde      = Bplus.posterior[,,s]
    #IR.1.tilde    = solve(B0.tilde)%*%B1.tilde%*%solve(B0.tilde)

    # Search for appropriate Q 
    sign.restrictions.do.not.hold = TRUE
    i=1
    while (sign.restrictions.do.not.hold){
      X           = matrix(rnorm(N^2),N,N)
      QR          = qr(X, tol = 1e-10)
      Q           = qr.Q(QR,complete=TRUE)
      R           = qr.R(QR,complete=TRUE)
      Q           = t(Q %*% diag(sign(diag(R))))
      B0          = Q%*%B0.tilde
      B1          = Q%*%B1.tilde
      B0.inv      = solve(B0)
      check       = prod(R1 %*% B0.inv %*% diag(N)[,shockvar] >= 0)
      A           = t(solve(B0)%*%B1)

      if (check==1){sign.restrictions.do.not.hold=FALSE}
      i=i+1
    }
    i.vec <- c(i.vec, i)
    Q.iden[,,s] <- Q
    B0.iden[,,s] <- B0
    B0.mean <- apply(B0.iden,1:2,mean)
    B1.iden[,,s] <- B1
    B1.mean <- apply(B1.iden,1:2,mean)
    A.iden[,,s] <- A
    A.mean <- apply(A.iden,1:2,mean)
  }

  re <- list("i" = i.vec, "Q" = Q.iden, "B0"= B0.iden, "B0.mean" = B0.mean,
             "Bplus"= B1.iden, "Bplus.mean" = B1.mean, "A" = A.iden, "A.mean" = A,
             "A.posterior"=A.posterior, "Sigma.posterior"=Sigma.posterior, "Theta"= Theta.mh)
  return(re)
  
}

```

Applying `v.posterior.mode` on the artifcial data yielded a mode of $\theta^{(s=1)}=(1,1,1,0.8)$ which is appropriate given that there is no COVID volatility in generated random walk series. However, the numerical optimization yielded a non-positive definite Hessian which cannot be used to initialize the Metropolis MCMC. As such, $W$ was instead set to an identity matrix.

Running `mh.mcmc` on the artificial data for 15,000 iterations yields the following draws. Unfortunately, the MCMC draws failed to converge into a stationary series within these number of iterations.

```{r RW Metropolis MCMC}
set.seed(1)
RW.post.mode <- v.posterior.mode(y, p = 1)
RW.mcmc <- mh.mcmc(y, c = 0.00001, W = diag(4), 
                   theta.init = RW.post.mode$maximizer, S.mh = 15000)
plot.ts(RW.mcmc$Theta, main = "Metropolis MCMC draws")
#print(RW.post.mode$maximizer) 
#print(RW.post.mode$hessian) 
#RM.theta$AcceptRate
```

Nonetheless, using these draws in the estimation procedure implemented by `sign.extension` still yields correct estimates for $A$ and $\Sigma$, as seen in the results below. Note however, memory capacity of the computer used only allowed for 2000 draws for the extended model.

(Note: 10,000 draws is possible for estimation using the actual macroeconomic data given the much fewer observations)

```{r RW extension, echo = F}
#| code-fold: true
#| code-summary: "Show code"
RW_estimation_ext <-  sign.extension(y, p = 1, S = 2000, 
                                     sign.restrictions=c(0, 0), k1 = 1, 
                                     Theta.mh = RW.mcmc$Theta[13001:15000,], shockvar = 1)

A.mean_tab <- as.data.frame(round(RW_estimation_ext$A.mean,0))
colnames(A.mean_tab) <- c("RW1", "RW2")

Sigma.mean_tab <- as.data.frame(round(apply(RW_estimation_ext$Sigma.posterior,1:2,mean),0))
colnames(Sigma.mean_tab) <- c("RW1", "RW2")

paged_table(A.mean_tab)
paged_table(Sigma.mean_tab)
```

## Empirical investigation

This section investigates the effect of an exogenous shock to public investment, represented by the public gross fixed capital formation (GFCF, i.e., the 5th variable in the $Y$). These will be measured through impulse response functions (IRFs) which capture the dynamic causal effects of an exogenous shock $u_t$ on the variables in the system.

Given the $VAR(1)$ representation of the model: $$Y_t = \textbf{A}Y_{t-1} + E_t$$ $$=E_t+\textbf{A}E_{t-1}+\textbf{A}^2E_{t-2}+...$$ Transforming back to a $VAR(p)$ representation using the $J$ matrix:

$$J=\big[I_n\quad 0_{N\times N(p-1)}\big]$$

$$y_t =JY_t=JE_t+J\textbf{A}J'JE_{t-1}+J\textbf{A}^2J'JE_{t-2}+...$$ $$=\varepsilon_t+J\textbf{A}J'\varepsilon_{t-1}+J\textbf{A}^2J'\varepsilon_{t-2}+...$$ Substituting $\varepsilon_t=Bu_t$:

$$y_t =Bu_t+J\textbf{A}J'Bu_{t-1}+J\textbf{A}^2J'Bu_{t-2}+...$$ $$=\Theta_0u_t+\Theta_1u_{t-1}+\Theta_2u_{t-2}+...$$Where matrix $\frac{\partial y_{t+i}}{\partial u_t}=\Theta_i$ is the IRF which characterizes the effect of the variables $y_t$, $i$ periods from impact.

The above derivations are implemented in the function `irf.plot` below, which takes in draws of $A$ and $B_0$ and does the necessary transformations estimate and plot the impluse response functions for a shock on a specified variable.

```{r IRF plot function, echo = TRUE}
#| code-fold: true
#| code-summary: "Show code"
irf.plot <- function(A.posterior, B0.posterior, shock.var, p=4, h = 12,
                     varnames = varname_vec){
  # Define colors
  ############################################################
  mcxs1  = "#05386B"
  mcxs2  = "#379683"
  mcxs3  = "#5CDB95"
  mcxs4  = "#8EE4AF"
  mcxs5  = "#EDF5E1"
  purple = "#b02442"
  
  mcxs1.rgb   = col2rgb(mcxs1)
  mcxs1.shade1= rgb(mcxs1.rgb[1],mcxs1.rgb[2],mcxs1.rgb[3], alpha=120, maxColorValue=255)
  mcxs2.rgb   = col2rgb(mcxs2)
  mcxs2.shade1= rgb(mcxs2.rgb[1],mcxs2.rgb[2],mcxs2.rgb[3], alpha=120, maxColorValue=255)
  
  # Impulse response functions
  ############################################################
  N <- dim(A.posterior)[2]
  S <- dim(A.posterior)[3]
  
  # transform B0 matrices to B
  B.posterior <- array(NA, c(N,N,S))
  for (s in 1:S){
  B.posterior[,,s] <- solve(B0.posterior[,,s])
  }
  
  IRF.posterior     = array(NA,c(N,N,h+1,S))
  IRF.inf.posterior = array(NA,c(N,N,S))
  J                 = cbind(diag(N),matrix(0,N,N*(p-1)))
  
  pb = txtProgressBar(min = 0, max = S, initial = 0) 
  for (s in 1:S){
    setTxtProgressBar(pb,s)
    # define A matrix in VAR(1) representation
    A.bold          = rbind(t(A.posterior[2:(1+N*p),,s]),cbind(diag(N*(p-1)),matrix(0,N*(p-1),N)))
    IRF.inf.posterior[,,s]          = J %*% solve(diag(N*p)-A.bold) %*% t(J) %*% B.posterior[,,s]
    A.bold.power    = A.bold
    for (i in 1:(h+1)){
      if (i==1){
        IRF.posterior[,,i,s]        = B.posterior[,,s]
      } else {
        IRF.posterior[,,i,s]        = J %*% A.bold.power %*% t(J) %*% B.posterior[,,s]
        A.bold.power                = A.bold.power %*% A.bold
      }
    }
  }
  
  # save IRFs
  save(IRF.posterior,IRF.inf.posterior, file="irf-k1.RData")

  # IRF plots GFCF
  ############################################################
  IRFs.k1           = apply(IRF.posterior[,shock.var,,],1:2,mean)
  IRFs.inf.k1       = apply(IRF.inf.posterior[,shock.var,],1,mean)
  rownames(IRFs.k1) = varnames
  
  IRFs.k1.hdi    = apply(IRF.posterior[,shock.var,,],1:2,hdi, credMass=0.68)
  hh          = 1:(h+1)
  
  par(mfrow=c(3,3), mar=c(3,3,2,2),cex.axis=1.5, cex.lab=1.5)
  for (n in 1:N){
    ylims     = range(IRFs.k1[n,hh],IRFs.k1.hdi[,n,hh])
    plot(hh,IRFs.k1[n,hh], type="l", ylim=ylims, axes=FALSE, xlab="3 years",
         main=rownames(IRFs.k1)[n])
    abline(h = 0, col = "firebrick")
    axis(2,c(ylims[1],0,ylims[2]),round(c(ylims[1],0,ylims[2]),3))
    polygon(c(hh,(h+1):1), c(IRFs.k1.hdi[1,n,hh],IRFs.k1.hdi[2,n,(h+1):1]),
            col=mcxs1.shade1,border=mcxs1.shade1)
    lines(hh, IRFs.k1[n,hh],lwd=2,col=mcxs1)
  }
}

```

### Basic model estimation and IRFs

The basic model is used to investigate the dynamic effects of shock to public investment on the economic system. Identification proceeds via the following sign restrictions on $\Theta_0$:

$$f(B_0,B_+)=\Theta_0=B=\begin{bmatrix}
* & * & * & * & * & * & * & * & *\\
* & * & * & * & * & * & * & * & *\\
+ & * & * & * & * & * & * & * & *\\
+ & * & * & * & * & * & * & * & *\\
+ & * & * & * & * & * & * & * & *\\
+ & * & * & * & * & * & * & * & *\\
+ & * & * & * & * & * & * & * & *\\
- & * & * & * & * & * & * & * & *\\
+ & * & * & * & * & * & * & * & *\\
\end{bmatrix}$$

which corresponds to the following $\textbf{R}_5$ matrix: $$\textbf{R}_5=\begin{bmatrix}
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & -1 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\
\end{bmatrix}$$

$$\textbf{R}_5f(B_0,B_+)e_5\geq0$$

Where the identifying fiscal policy impulse vector is: $$\text{diag}(\textbf{R}_5)=[0\quad0\quad1\quad1\quad1\quad1\quad1\quad-1\quad1]$$ $\text{diag}(\textbf{R}_5)$ represents an exogenous shock to public investment to which other fiscal variables respond positively and are accommodated by monetary policy. This scenario is applicable, for example during situations where the Australian economy is experiencing low inflation but sluggish growth which the government may want to stimulate in the short-to-medium term. Variables for unemployment and output, the first and second variables in $y_t$, respectively, are left unrestricted as these are the variables whose responses are of interest.

```{r Basic model estimation, echo = TRUE}
#| code-fold: true
#| code-summary: "Show code"

## interpretations ignoring recommendations of pagan and fry 2005. Not in terms of probability. 68% of IRF at a horizon are w/in the band. fraction of orthinognal IRFs. not stat significance. where are most of the orthogonal IRFs are. 

#expansionary fiscal policy, accommodated by monetary policy
basic.model <- sign.basic(lnY.df_num,p=4, S=10000, c(0, 0, 1, 1, 1, 1, 1, -1, 1), k1 = 1, shockvar=5)
irf.plot(A.posterior = basic.model$A.posterior, B0.posterior = basic.model$B0, shock.var = 5)
```

As seen in the above plots, an exogenous shock to public investment (i.e., government GFCF) at period $h=0$ is estimated to increase real GDP in subsequent periods, with the majority of estimated real GDP impulse responses at $h>0$ above the zero level. This expansion in output is accompanied by a decrease in unemployment. These results are consistent with the Keynesian economic theory that expansionary fiscal policy (through increased spending) increases output and employment. Note that these improvements to GDP and unemployment occur despite increases in tax revenue.

### Extended model estimation and IRFs

The same sign restrictions are used for identification in the extended model. Functions `v.posterior` and `mh.mcmc` are implemented to draw the COVID volatility vector $H$ based on the macroeconomic data. As seen in the plots below, the Metropolis MCMC also failed to converge to stationarity after 20,000 draws.

```{r Extended model MCMC, echo = TRUE}
#| code-fold: true
#| code-summary: "Show code"
extension.post.mode <- v.posterior.mode(lnY.df_num, p = 4, k1 = 1, k2 = 100)
set.seed(1)
extension.mcmc <-mh.mcmc(lnY.df_num, p=4, c=0.000228, W = solve(extension.post.mode$hessian), 
                theta.init = extension.post.mode$maximizer, k1 = 1, S.mh = 20000)
plot.ts(extension.mcmc$Theta, main = "Metropolis MCMC draws")

```

The resulting IRF plots from the extended model mimic the basic model results where an exogenous shock to GFCF results in higher output and lower unemployment rates. However, the bands surrounding the mean line of the IRFs have become slightly tighter, as accounting for COVID-induced conditional heteroskedasticity effectively down-weights the heightened volatility of observations during the pandemic.

```{r Extended model plots, echo = T}
#| code-fold: true
#| code-summary: "Show code"
extended.model <- sign.extension(lnY.df_num, p = 4, S = 10000, sign.restrictions=c(0, 0, 1, 1, 1, 1, 1, -1, 1), k1 = 1, 
                                 Theta.mh = extension.mcmc$Theta[10001:20000,])

irf.plot(A.posterior = extended.model$A.posterior, B0.posterior = extended.model$B0, shock.var = 5)
```

## References

Abubakar, Attahir B. (2016): Dynamic effects of fiscal policy on output and unemployment in Nigeria: An econometric investigation, CBN Journal of Applied Statistics, ISSN 2476-8472, The Central Bank of Nigeria, Abuja, Vol. 07, Iss. 2, pp. 101-122
